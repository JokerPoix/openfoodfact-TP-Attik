{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6ec0b7d9",
   "metadata": {},
   "source": [
    "# ü•£ 01 - Ingestion des donn√©es OpenFoodFacts\n",
    "Ce notebook a pour objectif de lire les donn√©es OpenFoodFacts brutes au format `.csv.gz`, de les analyser rapidement et de les convertir en format `Parquet` pour les √©tapes suivantes du pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8a07bf39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‚öôÔ∏è Installer pyspark si besoin (ex: sur Google Colab)\n",
    "try:\n",
    "    import pyspark\n",
    "except ImportError:\n",
    "    %pip install pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8240b1fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üì¶ Imports principaux\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6b42bfea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üöÄ Cr√©ation de la SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"OpenFoodFacts Ingestion\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "24d92544",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- code: double (nullable = true)\n",
      " |-- url: string (nullable = true)\n",
      " |-- creator: string (nullable = true)\n",
      " |-- created_t: integer (nullable = true)\n",
      " |-- created_datetime: timestamp (nullable = true)\n",
      " |-- last_modified_t: integer (nullable = true)\n",
      " |-- last_modified_datetime: timestamp (nullable = true)\n",
      " |-- last_modified_by: string (nullable = true)\n",
      " |-- last_updated_t: integer (nullable = true)\n",
      " |-- last_updated_datetime: timestamp (nullable = true)\n",
      " |-- product_name: string (nullable = true)\n",
      " |-- abbreviated_product_name: string (nullable = true)\n",
      " |-- generic_name: string (nullable = true)\n",
      " |-- quantity: string (nullable = true)\n",
      " |-- packaging: string (nullable = true)\n",
      " |-- packaging_tags: string (nullable = true)\n",
      " |-- packaging_en: string (nullable = true)\n",
      " |-- packaging_text: string (nullable = true)\n",
      " |-- brands: string (nullable = true)\n",
      " |-- brands_tags: string (nullable = true)\n",
      " |-- brands_en: string (nullable = true)\n",
      " |-- categories: string (nullable = true)\n",
      " |-- categories_tags: string (nullable = true)\n",
      " |-- categories_en: string (nullable = true)\n",
      " |-- origins: string (nullable = true)\n",
      " |-- origins_tags: string (nullable = true)\n",
      " |-- origins_en: string (nullable = true)\n",
      " |-- manufacturing_places: string (nullable = true)\n",
      " |-- manufacturing_places_tags: string (nullable = true)\n",
      " |-- labels: string (nullable = true)\n",
      " |-- labels_tags: string (nullable = true)\n",
      " |-- labels_en: string (nullable = true)\n",
      " |-- emb_codes: string (nullable = true)\n",
      " |-- emb_codes_tags: string (nullable = true)\n",
      " |-- first_packaging_code_geo: string (nullable = true)\n",
      " |-- cities: string (nullable = true)\n",
      " |-- cities_tags: string (nullable = true)\n",
      " |-- purchase_places: string (nullable = true)\n",
      " |-- stores: string (nullable = true)\n",
      " |-- countries: string (nullable = true)\n",
      " |-- countries_tags: string (nullable = true)\n",
      " |-- countries_en: string (nullable = true)\n",
      " |-- ingredients_text: string (nullable = true)\n",
      " |-- ingredients_tags: string (nullable = true)\n",
      " |-- ingredients_analysis_tags: string (nullable = true)\n",
      " |-- allergens: string (nullable = true)\n",
      " |-- allergens_en: string (nullable = true)\n",
      " |-- traces: string (nullable = true)\n",
      " |-- traces_tags: string (nullable = true)\n",
      " |-- traces_en: string (nullable = true)\n",
      " |-- serving_size: string (nullable = true)\n",
      " |-- serving_quantity: double (nullable = true)\n",
      " |-- no_nutrition_data: string (nullable = true)\n",
      " |-- additives_n: integer (nullable = true)\n",
      " |-- additives: string (nullable = true)\n",
      " |-- additives_tags: string (nullable = true)\n",
      " |-- additives_en: string (nullable = true)\n",
      " |-- nutriscore_score: integer (nullable = true)\n",
      " |-- nutriscore_grade: string (nullable = true)\n",
      " |-- nova_group: integer (nullable = true)\n",
      " |-- pnns_groups_1: string (nullable = true)\n",
      " |-- pnns_groups_2: string (nullable = true)\n",
      " |-- food_groups: string (nullable = true)\n",
      " |-- food_groups_tags: string (nullable = true)\n",
      " |-- food_groups_en: string (nullable = true)\n",
      " |-- states: string (nullable = true)\n",
      " |-- states_tags: string (nullable = true)\n",
      " |-- states_en: string (nullable = true)\n",
      " |-- brand_owner: string (nullable = true)\n",
      " |-- environmental_score_score: integer (nullable = true)\n",
      " |-- environmental_score_grade: string (nullable = true)\n",
      " |-- nutrient_levels_tags: string (nullable = true)\n",
      " |-- product_quantity: double (nullable = true)\n",
      " |-- owner: string (nullable = true)\n",
      " |-- data_quality_errors_tags: string (nullable = true)\n",
      " |-- unique_scans_n: integer (nullable = true)\n",
      " |-- popularity_tags: string (nullable = true)\n",
      " |-- completeness: double (nullable = true)\n",
      " |-- last_image_t: integer (nullable = true)\n",
      " |-- last_image_datetime: timestamp (nullable = true)\n",
      " |-- main_category: string (nullable = true)\n",
      " |-- main_category_en: string (nullable = true)\n",
      " |-- image_url: string (nullable = true)\n",
      " |-- image_small_url: string (nullable = true)\n",
      " |-- image_ingredients_url: string (nullable = true)\n",
      " |-- image_ingredients_small_url: string (nullable = true)\n",
      " |-- image_nutrition_url: string (nullable = true)\n",
      " |-- image_nutrition_small_url: string (nullable = true)\n",
      " |-- energy-kj_100g: double (nullable = true)\n",
      " |-- energy-kcal_100g: double (nullable = true)\n",
      " |-- energy_100g: double (nullable = true)\n",
      " |-- energy-from-fat_100g: double (nullable = true)\n",
      " |-- fat_100g: double (nullable = true)\n",
      " |-- saturated-fat_100g: double (nullable = true)\n",
      " |-- butyric-acid_100g: double (nullable = true)\n",
      " |-- caproic-acid_100g: double (nullable = true)\n",
      " |-- caprylic-acid_100g: double (nullable = true)\n",
      " |-- capric-acid_100g: double (nullable = true)\n",
      " |-- lauric-acid_100g: double (nullable = true)\n",
      " |-- myristic-acid_100g: double (nullable = true)\n",
      " |-- palmitic-acid_100g: double (nullable = true)\n",
      " |-- stearic-acid_100g: double (nullable = true)\n",
      " |-- arachidic-acid_100g: double (nullable = true)\n",
      " |-- behenic-acid_100g: double (nullable = true)\n",
      " |-- lignoceric-acid_100g: double (nullable = true)\n",
      " |-- cerotic-acid_100g: double (nullable = true)\n",
      " |-- montanic-acid_100g: double (nullable = true)\n",
      " |-- melissic-acid_100g: double (nullable = true)\n",
      " |-- unsaturated-fat_100g: double (nullable = true)\n",
      " |-- monounsaturated-fat_100g: double (nullable = true)\n",
      " |-- omega-9-fat_100g: double (nullable = true)\n",
      " |-- polyunsaturated-fat_100g: double (nullable = true)\n",
      " |-- omega-3-fat_100g: double (nullable = true)\n",
      " |-- omega-6-fat_100g: double (nullable = true)\n",
      " |-- alpha-linolenic-acid_100g: double (nullable = true)\n",
      " |-- eicosapentaenoic-acid_100g: double (nullable = true)\n",
      " |-- docosahexaenoic-acid_100g: double (nullable = true)\n",
      " |-- linoleic-acid_100g: double (nullable = true)\n",
      " |-- arachidonic-acid_100g: double (nullable = true)\n",
      " |-- gamma-linolenic-acid_100g: double (nullable = true)\n",
      " |-- dihomo-gamma-linolenic-acid_100g: double (nullable = true)\n",
      " |-- oleic-acid_100g: double (nullable = true)\n",
      " |-- elaidic-acid_100g: double (nullable = true)\n",
      " |-- gondoic-acid_100g: double (nullable = true)\n",
      " |-- mead-acid_100g: double (nullable = true)\n",
      " |-- erucic-acid_100g: double (nullable = true)\n",
      " |-- nervonic-acid_100g: double (nullable = true)\n",
      " |-- trans-fat_100g: double (nullable = true)\n",
      " |-- cholesterol_100g: double (nullable = true)\n",
      " |-- carbohydrates_100g: double (nullable = true)\n",
      " |-- sugars_100g: double (nullable = true)\n",
      " |-- added-sugars_100g: double (nullable = true)\n",
      " |-- sucrose_100g: double (nullable = true)\n",
      " |-- glucose_100g: double (nullable = true)\n",
      " |-- fructose_100g: double (nullable = true)\n",
      " |-- galactose_100g: double (nullable = true)\n",
      " |-- lactose_100g: double (nullable = true)\n",
      " |-- maltose_100g: double (nullable = true)\n",
      " |-- maltodextrins_100g: double (nullable = true)\n",
      " |-- starch_100g: double (nullable = true)\n",
      " |-- polyols_100g: double (nullable = true)\n",
      " |-- erythritol_100g: double (nullable = true)\n",
      " |-- fiber_100g: double (nullable = true)\n",
      " |-- soluble-fiber_100g: double (nullable = true)\n",
      " |-- insoluble-fiber_100g: double (nullable = true)\n",
      " |-- proteins_100g: double (nullable = true)\n",
      " |-- casein_100g: double (nullable = true)\n",
      " |-- serum-proteins_100g: double (nullable = true)\n",
      " |-- nucleotides_100g: double (nullable = true)\n",
      " |-- salt_100g: double (nullable = true)\n",
      " |-- added-salt_100g: double (nullable = true)\n",
      " |-- sodium_100g: double (nullable = true)\n",
      " |-- alcohol_100g: double (nullable = true)\n",
      " |-- vitamin-a_100g: double (nullable = true)\n",
      " |-- beta-carotene_100g: double (nullable = true)\n",
      " |-- vitamin-d_100g: double (nullable = true)\n",
      " |-- vitamin-e_100g: double (nullable = true)\n",
      " |-- vitamin-k_100g: double (nullable = true)\n",
      " |-- vitamin-c_100g: double (nullable = true)\n",
      " |-- vitamin-b1_100g: double (nullable = true)\n",
      " |-- vitamin-b2_100g: double (nullable = true)\n",
      " |-- vitamin-pp_100g: double (nullable = true)\n",
      " |-- vitamin-b6_100g: double (nullable = true)\n",
      " |-- vitamin-b9_100g: double (nullable = true)\n",
      " |-- folates_100g: double (nullable = true)\n",
      " |-- vitamin-b12_100g: double (nullable = true)\n",
      " |-- biotin_100g: double (nullable = true)\n",
      " |-- pantothenic-acid_100g: double (nullable = true)\n",
      " |-- silica_100g: double (nullable = true)\n",
      " |-- bicarbonate_100g: double (nullable = true)\n",
      " |-- potassium_100g: double (nullable = true)\n",
      " |-- chloride_100g: double (nullable = true)\n",
      " |-- calcium_100g: double (nullable = true)\n",
      " |-- phosphorus_100g: double (nullable = true)\n",
      " |-- iron_100g: double (nullable = true)\n",
      " |-- magnesium_100g: double (nullable = true)\n",
      " |-- zinc_100g: double (nullable = true)\n",
      " |-- copper_100g: double (nullable = true)\n",
      " |-- manganese_100g: double (nullable = true)\n",
      " |-- fluoride_100g: double (nullable = true)\n",
      " |-- selenium_100g: double (nullable = true)\n",
      " |-- chromium_100g: double (nullable = true)\n",
      " |-- molybdenum_100g: double (nullable = true)\n",
      " |-- iodine_100g: double (nullable = true)\n",
      " |-- caffeine_100g: double (nullable = true)\n",
      " |-- taurine_100g: double (nullable = true)\n",
      " |-- ph_100g: double (nullable = true)\n",
      " |-- fruits-vegetables-nuts_100g: double (nullable = true)\n",
      " |-- fruits-vegetables-nuts-dried_100g: double (nullable = true)\n",
      " |-- fruits-vegetables-nuts-estimate_100g: double (nullable = true)\n",
      " |-- fruits-vegetables-nuts-estimate-from-ingredients_100g: double (nullable = true)\n",
      " |-- collagen-meat-protein-ratio_100g: double (nullable = true)\n",
      " |-- cocoa_100g: double (nullable = true)\n",
      " |-- chlorophyl_100g: double (nullable = true)\n",
      " |-- carbon-footprint_100g: double (nullable = true)\n",
      " |-- carbon-footprint-from-meat-or-fish_100g: double (nullable = true)\n",
      " |-- nutrition-score-fr_100g: integer (nullable = true)\n",
      " |-- nutrition-score-uk_100g: integer (nullable = true)\n",
      " |-- glycemic-index_100g: double (nullable = true)\n",
      " |-- water-hardness_100g: double (nullable = true)\n",
      " |-- choline_100g: double (nullable = true)\n",
      " |-- phylloquinone_100g: double (nullable = true)\n",
      " |-- beta-glucan_100g: double (nullable = true)\n",
      " |-- inositol_100g: double (nullable = true)\n",
      " |-- carnitine_100g: double (nullable = true)\n",
      " |-- sulphate_100g: double (nullable = true)\n",
      " |-- nitrate_100g: double (nullable = true)\n",
      " |-- acidity_100g: double (nullable = true)\n",
      " |-- carbohydrates-total_100g: double (nullable = true)\n"
     ]
    }
   ],
   "source": [
    "# üì• Lecture du fichier .csv.gz (format TSV)\n",
    "input_path = \"../data/en.openfoodfacts.org.products.csv.gz\"\n",
    "\n",
    "df_raw = spark.read.option(\"header\", True) \\\n",
    "                   .option(\"sep\", \"\\t\") \\\n",
    "                   .option(\"inferSchema\", True) \\\n",
    "                   .csv(input_path)\n",
    "\n",
    "df_raw.cache()\n",
    "df_raw.printSchema()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2ad32c0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nombre de lignes: 3,904,751\n",
      "Nombre de colonnes: 209\n"
     ]
    }
   ],
   "source": [
    "# üî¢ Dimensions du DataFrame\n",
    "n_rows = df_raw.count()\n",
    "n_cols = len(df_raw.columns)\n",
    "print(f\"Nombre de lignes: {n_rows:,}\")\n",
    "print(f\"Nombre de colonnes: {n_cols}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a145c17c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üßæ Liste des colonnes :\n",
      "01. code\n",
      "02. url\n",
      "03. creator\n",
      "04. created_t\n",
      "05. created_datetime\n",
      "06. last_modified_t\n",
      "07. last_modified_datetime\n",
      "08. last_modified_by\n",
      "09. last_updated_t\n",
      "10. last_updated_datetime\n",
      "11. product_name\n",
      "12. abbreviated_product_name\n",
      "13. generic_name\n",
      "14. quantity\n",
      "15. packaging\n",
      "16. packaging_tags\n",
      "17. packaging_en\n",
      "18. packaging_text\n",
      "19. brands\n",
      "20. brands_tags\n",
      "21. brands_en\n",
      "22. categories\n",
      "23. categories_tags\n",
      "24. categories_en\n",
      "25. origins\n",
      "26. origins_tags\n",
      "27. origins_en\n",
      "28. manufacturing_places\n",
      "29. manufacturing_places_tags\n",
      "30. labels\n",
      "31. labels_tags\n",
      "32. labels_en\n",
      "33. emb_codes\n",
      "34. emb_codes_tags\n",
      "35. first_packaging_code_geo\n",
      "36. cities\n",
      "37. cities_tags\n",
      "38. purchase_places\n",
      "39. stores\n",
      "40. countries\n",
      "41. countries_tags\n",
      "42. countries_en\n",
      "43. ingredients_text\n",
      "44. ingredients_tags\n",
      "45. ingredients_analysis_tags\n",
      "46. allergens\n",
      "47. allergens_en\n",
      "48. traces\n",
      "49. traces_tags\n",
      "50. traces_en\n",
      "51. serving_size\n",
      "52. serving_quantity\n",
      "53. no_nutrition_data\n",
      "54. additives_n\n",
      "55. additives\n",
      "56. additives_tags\n",
      "57. additives_en\n",
      "58. nutriscore_score\n",
      "59. nutriscore_grade\n",
      "60. nova_group\n",
      "61. pnns_groups_1\n",
      "62. pnns_groups_2\n",
      "63. food_groups\n",
      "64. food_groups_tags\n",
      "65. food_groups_en\n",
      "66. states\n",
      "67. states_tags\n",
      "68. states_en\n",
      "69. brand_owner\n",
      "70. environmental_score_score\n",
      "71. environmental_score_grade\n",
      "72. nutrient_levels_tags\n",
      "73. product_quantity\n",
      "74. owner\n",
      "75. data_quality_errors_tags\n",
      "76. unique_scans_n\n",
      "77. popularity_tags\n",
      "78. completeness\n",
      "79. last_image_t\n",
      "80. last_image_datetime\n",
      "81. main_category\n",
      "82. main_category_en\n",
      "83. image_url\n",
      "84. image_small_url\n",
      "85. image_ingredients_url\n",
      "86. image_ingredients_small_url\n",
      "87. image_nutrition_url\n",
      "88. image_nutrition_small_url\n",
      "89. energy-kj_100g\n",
      "90. energy-kcal_100g\n",
      "91. energy_100g\n",
      "92. energy-from-fat_100g\n",
      "93. fat_100g\n",
      "94. saturated-fat_100g\n",
      "95. butyric-acid_100g\n",
      "96. caproic-acid_100g\n",
      "97. caprylic-acid_100g\n",
      "98. capric-acid_100g\n",
      "99. lauric-acid_100g\n",
      "100. myristic-acid_100g\n",
      "101. palmitic-acid_100g\n",
      "102. stearic-acid_100g\n",
      "103. arachidic-acid_100g\n",
      "104. behenic-acid_100g\n",
      "105. lignoceric-acid_100g\n",
      "106. cerotic-acid_100g\n",
      "107. montanic-acid_100g\n",
      "108. melissic-acid_100g\n",
      "109. unsaturated-fat_100g\n",
      "110. monounsaturated-fat_100g\n",
      "111. omega-9-fat_100g\n",
      "112. polyunsaturated-fat_100g\n",
      "113. omega-3-fat_100g\n",
      "114. omega-6-fat_100g\n",
      "115. alpha-linolenic-acid_100g\n",
      "116. eicosapentaenoic-acid_100g\n",
      "117. docosahexaenoic-acid_100g\n",
      "118. linoleic-acid_100g\n",
      "119. arachidonic-acid_100g\n",
      "120. gamma-linolenic-acid_100g\n",
      "121. dihomo-gamma-linolenic-acid_100g\n",
      "122. oleic-acid_100g\n",
      "123. elaidic-acid_100g\n",
      "124. gondoic-acid_100g\n",
      "125. mead-acid_100g\n",
      "126. erucic-acid_100g\n",
      "127. nervonic-acid_100g\n",
      "128. trans-fat_100g\n",
      "129. cholesterol_100g\n",
      "130. carbohydrates_100g\n",
      "131. sugars_100g\n",
      "132. added-sugars_100g\n",
      "133. sucrose_100g\n",
      "134. glucose_100g\n",
      "135. fructose_100g\n",
      "136. galactose_100g\n",
      "137. lactose_100g\n",
      "138. maltose_100g\n",
      "139. maltodextrins_100g\n",
      "140. starch_100g\n",
      "141. polyols_100g\n",
      "142. erythritol_100g\n",
      "143. fiber_100g\n",
      "144. soluble-fiber_100g\n",
      "145. insoluble-fiber_100g\n",
      "146. proteins_100g\n",
      "147. casein_100g\n",
      "148. serum-proteins_100g\n",
      "149. nucleotides_100g\n",
      "150. salt_100g\n",
      "151. added-salt_100g\n",
      "152. sodium_100g\n",
      "153. alcohol_100g\n",
      "154. vitamin-a_100g\n",
      "155. beta-carotene_100g\n",
      "156. vitamin-d_100g\n",
      "157. vitamin-e_100g\n",
      "158. vitamin-k_100g\n",
      "159. vitamin-c_100g\n",
      "160. vitamin-b1_100g\n",
      "161. vitamin-b2_100g\n",
      "162. vitamin-pp_100g\n",
      "163. vitamin-b6_100g\n",
      "164. vitamin-b9_100g\n",
      "165. folates_100g\n",
      "166. vitamin-b12_100g\n",
      "167. biotin_100g\n",
      "168. pantothenic-acid_100g\n",
      "169. silica_100g\n",
      "170. bicarbonate_100g\n",
      "171. potassium_100g\n",
      "172. chloride_100g\n",
      "173. calcium_100g\n",
      "174. phosphorus_100g\n",
      "175. iron_100g\n",
      "176. magnesium_100g\n",
      "177. zinc_100g\n",
      "178. copper_100g\n",
      "179. manganese_100g\n",
      "180. fluoride_100g\n",
      "181. selenium_100g\n",
      "182. chromium_100g\n",
      "183. molybdenum_100g\n",
      "184. iodine_100g\n",
      "185. caffeine_100g\n",
      "186. taurine_100g\n",
      "187. ph_100g\n",
      "188. fruits-vegetables-nuts_100g\n",
      "189. fruits-vegetables-nuts-dried_100g\n",
      "190. fruits-vegetables-nuts-estimate_100g\n",
      "191. fruits-vegetables-nuts-estimate-from-ingredients_100g\n",
      "192. collagen-meat-protein-ratio_100g\n",
      "193. cocoa_100g\n",
      "194. chlorophyl_100g\n",
      "195. carbon-footprint_100g\n",
      "196. carbon-footprint-from-meat-or-fish_100g\n",
      "197. nutrition-score-fr_100g\n",
      "198. nutrition-score-uk_100g\n",
      "199. glycemic-index_100g\n",
      "200. water-hardness_100g\n",
      "201. choline_100g\n",
      "202. phylloquinone_100g\n",
      "203. beta-glucan_100g\n",
      "204. inositol_100g\n",
      "205. carnitine_100g\n",
      "206. sulphate_100g\n",
      "207. nitrate_100g\n",
      "208. acidity_100g\n",
      "209. carbohydrates-total_100g\n"
     ]
    }
   ],
   "source": [
    "# üìã Affichage des noms de colonnes\n",
    "print(\"\\nüßæ Liste des colonnes :\")\n",
    "for i, col_name in enumerate(df_raw.columns, start=1):\n",
    "    print(f\"{i:02d}. {col_name}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e352ec08-21ff-469b-87ee-e3971bf1a193",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2d021f69",
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o41.csv.\n: java.lang.RuntimeException: java.io.FileNotFoundException: java.io.FileNotFoundException: HADOOP_HOME and hadoop.home.dir are unset. -see https://cwiki.apache.org/confluence/display/HADOOP2/WindowsProblems\r\n\tat org.apache.hadoop.util.Shell.getWinUtilsPath(Shell.java:789)\r\n\tat org.apache.hadoop.util.Shell.getSetPermissionCommand(Shell.java:298)\r\n\tat org.apache.hadoop.util.Shell.getSetPermissionCommand(Shell.java:314)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.setPermission(RawLocalFileSystem.java:1116)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.mkOneDirWithMode(RawLocalFileSystem.java:798)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirsWithOptionalPermission(RawLocalFileSystem.java:838)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirs(RawLocalFileSystem.java:810)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirsWithOptionalPermission(RawLocalFileSystem.java:837)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirs(RawLocalFileSystem.java:810)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirsWithOptionalPermission(RawLocalFileSystem.java:837)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirs(RawLocalFileSystem.java:810)\r\n\tat org.apache.hadoop.fs.ChecksumFileSystem.mkdirs(ChecksumFileSystem.java:988)\r\n\tat org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.setupJob(FileOutputCommitter.java:356)\r\n\tat org.apache.spark.internal.io.HadoopMapReduceCommitProtocol.setupJob(HadoopMapReduceCommitProtocol.scala:190)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.writeAndCommit(FileFormatWriter.scala:268)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeWrite(FileFormatWriter.scala:306)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:189)\r\n\tat org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:195)\r\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:117)\r\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:115)\r\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.executeCollect(commands.scala:129)\r\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$2(QueryExecution.scala:155)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$8(SQLExecution.scala:162)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withSessionTagsApplied(SQLExecution.scala:268)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$7(SQLExecution.scala:124)\r\n\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)\r\n\tat org.apache.spark.sql.artifact.ArtifactManager.$anonfun$withResources$1(ArtifactManager.scala:112)\r\n\tat org.apache.spark.sql.artifact.ArtifactManager.withClassLoaderIfNeeded(ArtifactManager.scala:106)\r\n\tat org.apache.spark.sql.artifact.ArtifactManager.withResources(ArtifactManager.scala:111)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$6(SQLExecution.scala:124)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:291)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$1(SQLExecution.scala:123)\r\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:804)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId0(SQLExecution.scala:77)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:233)\r\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$1(QueryExecution.scala:155)\r\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:654)\r\n\tat org.apache.spark.sql.execution.QueryExecution.org$apache$spark$sql$execution$QueryExecution$$eagerlyExecute$1(QueryExecution.scala:154)\r\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$3.applyOrElse(QueryExecution.scala:169)\r\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$3.applyOrElse(QueryExecution.scala:164)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:470)\r\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:86)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:470)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:37)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:360)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:356)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:37)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:37)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:446)\r\n\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:164)\r\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyCommandExecuted$1(QueryExecution.scala:126)\r\n\tat scala.util.Try$.apply(Try.scala:217)\r\n\tat org.apache.spark.util.Utils$.doTryWithCallerStacktrace(Utils.scala:1378)\r\n\tat org.apache.spark.util.Utils$.getTryWithCallerStacktrace(Utils.scala:1439)\r\n\tat org.apache.spark.util.LazyTry.get(LazyTry.scala:58)\r\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:131)\r\n\tat org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:192)\r\n\tat org.apache.spark.sql.classic.DataFrameWriter.runCommand(DataFrameWriter.scala:622)\r\n\tat org.apache.spark.sql.classic.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:273)\r\n\tat org.apache.spark.sql.classic.DataFrameWriter.saveInternal(DataFrameWriter.scala:241)\r\n\tat org.apache.spark.sql.classic.DataFrameWriter.save(DataFrameWriter.scala:118)\r\n\tat org.apache.spark.sql.DataFrameWriter.csv(DataFrameWriter.scala:426)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\r\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:569)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:184)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:108)\r\n\tat java.base/java.lang.Thread.run(Thread.java:840)\r\n\tSuppressed: org.apache.spark.util.Utils$OriginalTryStackTraceException: Full stacktrace of original doTryWithCallerStacktrace caller\r\n\t\tat org.apache.hadoop.util.Shell.getWinUtilsPath(Shell.java:789)\r\n\t\tat org.apache.hadoop.util.Shell.getSetPermissionCommand(Shell.java:298)\r\n\t\tat org.apache.hadoop.util.Shell.getSetPermissionCommand(Shell.java:314)\r\n\t\tat org.apache.hadoop.fs.RawLocalFileSystem.setPermission(RawLocalFileSystem.java:1116)\r\n\t\tat org.apache.hadoop.fs.RawLocalFileSystem.mkOneDirWithMode(RawLocalFileSystem.java:798)\r\n\t\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirsWithOptionalPermission(RawLocalFileSystem.java:838)\r\n\t\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirs(RawLocalFileSystem.java:810)\r\n\t\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirsWithOptionalPermission(RawLocalFileSystem.java:837)\r\n\t\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirs(RawLocalFileSystem.java:810)\r\n\t\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirsWithOptionalPermission(RawLocalFileSystem.java:837)\r\n\t\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirs(RawLocalFileSystem.java:810)\r\n\t\tat org.apache.hadoop.fs.ChecksumFileSystem.mkdirs(ChecksumFileSystem.java:988)\r\n\t\tat org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.setupJob(FileOutputCommitter.java:356)\r\n\t\tat org.apache.spark.internal.io.HadoopMapReduceCommitProtocol.setupJob(HadoopMapReduceCommitProtocol.scala:190)\r\n\t\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.writeAndCommit(FileFormatWriter.scala:268)\r\n\t\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeWrite(FileFormatWriter.scala:306)\r\n\t\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:189)\r\n\t\tat org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:195)\r\n\t\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:117)\r\n\t\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:115)\r\n\t\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.executeCollect(commands.scala:129)\r\n\t\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$2(QueryExecution.scala:155)\r\n\t\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$8(SQLExecution.scala:162)\r\n\t\tat org.apache.spark.sql.execution.SQLExecution$.withSessionTagsApplied(SQLExecution.scala:268)\r\n\t\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$7(SQLExecution.scala:124)\r\n\t\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)\r\n\t\tat org.apache.spark.sql.artifact.ArtifactManager.$anonfun$withResources$1(ArtifactManager.scala:112)\r\n\t\tat org.apache.spark.sql.artifact.ArtifactManager.withClassLoaderIfNeeded(ArtifactManager.scala:106)\r\n\t\tat org.apache.spark.sql.artifact.ArtifactManager.withResources(ArtifactManager.scala:111)\r\n\t\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$6(SQLExecution.scala:124)\r\n\t\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:291)\r\n\t\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$1(SQLExecution.scala:123)\r\n\t\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:804)\r\n\t\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId0(SQLExecution.scala:77)\r\n\t\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:233)\r\n\t\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$1(QueryExecution.scala:155)\r\n\t\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:654)\r\n\t\tat org.apache.spark.sql.execution.QueryExecution.org$apache$spark$sql$execution$QueryExecution$$eagerlyExecute$1(QueryExecution.scala:154)\r\n\t\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$3.applyOrElse(QueryExecution.scala:169)\r\n\t\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$3.applyOrElse(QueryExecution.scala:164)\r\n\t\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:470)\r\n\t\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:86)\r\n\t\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:470)\r\n\t\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:37)\r\n\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:360)\r\n\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:356)\r\n\t\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:37)\r\n\t\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:37)\r\n\t\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:446)\r\n\t\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:164)\r\n\t\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyCommandExecuted$1(QueryExecution.scala:126)\r\n\t\tat scala.util.Try$.apply(Try.scala:217)\r\n\t\tat org.apache.spark.util.Utils$.doTryWithCallerStacktrace(Utils.scala:1378)\r\n\t\tat org.apache.spark.util.LazyTry.tryT$lzycompute(LazyTry.scala:46)\r\n\t\tat org.apache.spark.util.LazyTry.tryT(LazyTry.scala:46)\r\n\t\t... 20 more\r\nCaused by: java.io.FileNotFoundException: java.io.FileNotFoundException: HADOOP_HOME and hadoop.home.dir are unset. -see https://cwiki.apache.org/confluence/display/HADOOP2/WindowsProblems\r\n\tat org.apache.hadoop.util.Shell.fileNotFoundException(Shell.java:601)\r\n\tat org.apache.hadoop.util.Shell.getHadoopHomeDir(Shell.java:622)\r\n\tat org.apache.hadoop.util.Shell.getQualifiedBin(Shell.java:645)\r\n\tat org.apache.hadoop.util.Shell.<clinit>(Shell.java:742)\r\n\tat org.apache.hadoop.util.StringUtils.<clinit>(StringUtils.java:80)\r\n\tat org.apache.hadoop.conf.Configuration.getTimeDurationHelper(Configuration.java:1954)\r\n\tat org.apache.hadoop.conf.Configuration.getTimeDuration(Configuration.java:1912)\r\n\tat org.apache.hadoop.conf.Configuration.getTimeDuration(Configuration.java:1885)\r\n\tat org.apache.hadoop.util.ShutdownHookManager.getShutdownTimeout(ShutdownHookManager.java:183)\r\n\tat org.apache.hadoop.util.ShutdownHookManager$HookEntry.<init>(ShutdownHookManager.java:207)\r\n\tat org.apache.hadoop.util.ShutdownHookManager.addShutdownHook(ShutdownHookManager.java:304)\r\n\tat org.apache.spark.util.SparkShutdownHookManager.$anonfun$install$1(ShutdownHookManager.scala:194)\r\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\r\n\tat scala.Option.fold(Option.scala:263)\r\n\tat org.apache.spark.util.SparkShutdownHookManager.install(ShutdownHookManager.scala:195)\r\n\tat org.apache.spark.util.ShutdownHookManager$.shutdownHooks$lzycompute(ShutdownHookManager.scala:55)\r\n\tat org.apache.spark.util.ShutdownHookManager$.shutdownHooks(ShutdownHookManager.scala:53)\r\n\tat org.apache.spark.util.ShutdownHookManager$.addShutdownHook(ShutdownHookManager.scala:159)\r\n\tat org.apache.spark.util.ShutdownHookManager$.<clinit>(ShutdownHookManager.scala:63)\r\n\tat org.apache.spark.util.Utils$.createTempDir(Utils.scala:250)\r\n\tat org.apache.spark.util.SparkFileUtils.createTempDir(SparkFileUtils.scala:103)\r\n\tat org.apache.spark.util.SparkFileUtils.createTempDir$(SparkFileUtils.scala:102)\r\n\tat org.apache.spark.util.Utils$.createTempDir(Utils.scala:99)\r\n\tat org.apache.spark.deploy.SparkSubmit.prepareSubmitEnvironment(SparkSubmit.scala:379)\r\n\tat org.apache.spark.deploy.SparkSubmit.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:961)\r\n\tat org.apache.spark.deploy.SparkSubmit.doRunMain$1(SparkSubmit.scala:204)\r\n\tat org.apache.spark.deploy.SparkSubmit.submit(SparkSubmit.scala:227)\r\n\tat org.apache.spark.deploy.SparkSubmit.doSubmit(SparkSubmit.scala:96)\r\n\tat org.apache.spark.deploy.SparkSubmit$$anon$2.doSubmit(SparkSubmit.scala:1132)\r\n\tat org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:1141)\r\n\tat org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)\r\nCaused by: java.io.FileNotFoundException: HADOOP_HOME and hadoop.home.dir are unset.\r\n\tat org.apache.hadoop.util.Shell.checkHadoopHomeInner(Shell.java:521)\r\n\tat org.apache.hadoop.util.Shell.checkHadoopHome(Shell.java:492)\r\n\tat org.apache.hadoop.util.Shell.<clinit>(Shell.java:569)\r\n\t... 27 more\r\n",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mPy4JJavaError\u001B[0m                             Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[7], line 9\u001B[0m\n\u001B[0;32m      6\u001B[0m os\u001B[38;5;241m.\u001B[39mmakedirs(output_dir, exist_ok\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m)\n\u001B[0;32m      8\u001B[0m \u001B[38;5;66;03m# √âcriture au format CSV (avec header)\u001B[39;00m\n\u001B[1;32m----> 9\u001B[0m \u001B[43mdf_raw\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mwrite\u001B[49m\u001B[43m \u001B[49m\u001B[43m\\\u001B[49m\n\u001B[0;32m     10\u001B[0m \u001B[38;5;241;43m.\u001B[39;49m\u001B[43moption\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mheader\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mtrue\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m \u001B[49m\u001B[43m\\\u001B[49m\n\u001B[0;32m     11\u001B[0m \u001B[38;5;241;43m.\u001B[39;49m\u001B[43moption\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43msep\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43m;\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m\\\u001B[49m\n\u001B[0;32m     12\u001B[0m \u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmode\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43moverwrite\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m \u001B[49m\u001B[43m\\\u001B[49m\n\u001B[0;32m     13\u001B[0m \u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcsv\u001B[49m\u001B[43m(\u001B[49m\u001B[43moutput_dir\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     15\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m‚úÖ Ingestion CSV termin√©e dans : \u001B[39m\u001B[38;5;132;01m{\u001B[39;00moutput_dir\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n",
      "File \u001B[1;32m~\\Documents\\PycharmProjects\\openfoodfact-TP-Attik\\venv\\lib\\site-packages\\pyspark\\sql\\readwriter.py:2146\u001B[0m, in \u001B[0;36mDataFrameWriter.csv\u001B[1;34m(self, path, mode, compression, sep, quote, escape, header, nullValue, escapeQuotes, quoteAll, dateFormat, timestampFormat, ignoreLeadingWhiteSpace, ignoreTrailingWhiteSpace, charToEscapeQuoteEscaping, encoding, emptyValue, lineSep)\u001B[0m\n\u001B[0;32m   2127\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmode(mode)\n\u001B[0;32m   2128\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_set_opts(\n\u001B[0;32m   2129\u001B[0m     compression\u001B[38;5;241m=\u001B[39mcompression,\n\u001B[0;32m   2130\u001B[0m     sep\u001B[38;5;241m=\u001B[39msep,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m   2144\u001B[0m     lineSep\u001B[38;5;241m=\u001B[39mlineSep,\n\u001B[0;32m   2145\u001B[0m )\n\u001B[1;32m-> 2146\u001B[0m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_jwrite\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcsv\u001B[49m\u001B[43m(\u001B[49m\u001B[43mpath\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\Documents\\PycharmProjects\\openfoodfact-TP-Attik\\venv\\lib\\site-packages\\py4j\\java_gateway.py:1362\u001B[0m, in \u001B[0;36mJavaMember.__call__\u001B[1;34m(self, *args)\u001B[0m\n\u001B[0;32m   1356\u001B[0m command \u001B[38;5;241m=\u001B[39m proto\u001B[38;5;241m.\u001B[39mCALL_COMMAND_NAME \u001B[38;5;241m+\u001B[39m\\\n\u001B[0;32m   1357\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcommand_header \u001B[38;5;241m+\u001B[39m\\\n\u001B[0;32m   1358\u001B[0m     args_command \u001B[38;5;241m+\u001B[39m\\\n\u001B[0;32m   1359\u001B[0m     proto\u001B[38;5;241m.\u001B[39mEND_COMMAND_PART\n\u001B[0;32m   1361\u001B[0m answer \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgateway_client\u001B[38;5;241m.\u001B[39msend_command(command)\n\u001B[1;32m-> 1362\u001B[0m return_value \u001B[38;5;241m=\u001B[39m \u001B[43mget_return_value\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m   1363\u001B[0m \u001B[43m    \u001B[49m\u001B[43manswer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgateway_client\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtarget_id\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mname\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1365\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m temp_arg \u001B[38;5;129;01min\u001B[39;00m temp_args:\n\u001B[0;32m   1366\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mhasattr\u001B[39m(temp_arg, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m_detach\u001B[39m\u001B[38;5;124m\"\u001B[39m):\n",
      "File \u001B[1;32m~\\Documents\\PycharmProjects\\openfoodfact-TP-Attik\\venv\\lib\\site-packages\\pyspark\\errors\\exceptions\\captured.py:282\u001B[0m, in \u001B[0;36mcapture_sql_exception.<locals>.deco\u001B[1;34m(*a, **kw)\u001B[0m\n\u001B[0;32m    279\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21;01mpy4j\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mprotocol\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m Py4JJavaError\n\u001B[0;32m    281\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m--> 282\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m f(\u001B[38;5;241m*\u001B[39ma, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkw)\n\u001B[0;32m    283\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m Py4JJavaError \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[0;32m    284\u001B[0m     converted \u001B[38;5;241m=\u001B[39m convert_exception(e\u001B[38;5;241m.\u001B[39mjava_exception)\n",
      "File \u001B[1;32m~\\Documents\\PycharmProjects\\openfoodfact-TP-Attik\\venv\\lib\\site-packages\\py4j\\protocol.py:327\u001B[0m, in \u001B[0;36mget_return_value\u001B[1;34m(answer, gateway_client, target_id, name)\u001B[0m\n\u001B[0;32m    325\u001B[0m value \u001B[38;5;241m=\u001B[39m OUTPUT_CONVERTER[\u001B[38;5;28mtype\u001B[39m](answer[\u001B[38;5;241m2\u001B[39m:], gateway_client)\n\u001B[0;32m    326\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m answer[\u001B[38;5;241m1\u001B[39m] \u001B[38;5;241m==\u001B[39m REFERENCE_TYPE:\n\u001B[1;32m--> 327\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m Py4JJavaError(\n\u001B[0;32m    328\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mAn error occurred while calling \u001B[39m\u001B[38;5;132;01m{0}\u001B[39;00m\u001B[38;5;132;01m{1}\u001B[39;00m\u001B[38;5;132;01m{2}\u001B[39;00m\u001B[38;5;124m.\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;241m.\u001B[39m\n\u001B[0;32m    329\u001B[0m         \u001B[38;5;28mformat\u001B[39m(target_id, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m.\u001B[39m\u001B[38;5;124m\"\u001B[39m, name), value)\n\u001B[0;32m    330\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m    331\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m Py4JError(\n\u001B[0;32m    332\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mAn error occurred while calling \u001B[39m\u001B[38;5;132;01m{0}\u001B[39;00m\u001B[38;5;132;01m{1}\u001B[39;00m\u001B[38;5;132;01m{2}\u001B[39;00m\u001B[38;5;124m. Trace:\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;132;01m{3}\u001B[39;00m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;241m.\u001B[39m\n\u001B[0;32m    333\u001B[0m         \u001B[38;5;28mformat\u001B[39m(target_id, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m.\u001B[39m\u001B[38;5;124m\"\u001B[39m, name, value))\n",
      "\u001B[1;31mPy4JJavaError\u001B[0m: An error occurred while calling o41.csv.\n: java.lang.RuntimeException: java.io.FileNotFoundException: java.io.FileNotFoundException: HADOOP_HOME and hadoop.home.dir are unset. -see https://cwiki.apache.org/confluence/display/HADOOP2/WindowsProblems\r\n\tat org.apache.hadoop.util.Shell.getWinUtilsPath(Shell.java:789)\r\n\tat org.apache.hadoop.util.Shell.getSetPermissionCommand(Shell.java:298)\r\n\tat org.apache.hadoop.util.Shell.getSetPermissionCommand(Shell.java:314)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.setPermission(RawLocalFileSystem.java:1116)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.mkOneDirWithMode(RawLocalFileSystem.java:798)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirsWithOptionalPermission(RawLocalFileSystem.java:838)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirs(RawLocalFileSystem.java:810)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirsWithOptionalPermission(RawLocalFileSystem.java:837)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirs(RawLocalFileSystem.java:810)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirsWithOptionalPermission(RawLocalFileSystem.java:837)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirs(RawLocalFileSystem.java:810)\r\n\tat org.apache.hadoop.fs.ChecksumFileSystem.mkdirs(ChecksumFileSystem.java:988)\r\n\tat org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.setupJob(FileOutputCommitter.java:356)\r\n\tat org.apache.spark.internal.io.HadoopMapReduceCommitProtocol.setupJob(HadoopMapReduceCommitProtocol.scala:190)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.writeAndCommit(FileFormatWriter.scala:268)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeWrite(FileFormatWriter.scala:306)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:189)\r\n\tat org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:195)\r\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:117)\r\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:115)\r\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.executeCollect(commands.scala:129)\r\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$2(QueryExecution.scala:155)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$8(SQLExecution.scala:162)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withSessionTagsApplied(SQLExecution.scala:268)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$7(SQLExecution.scala:124)\r\n\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)\r\n\tat org.apache.spark.sql.artifact.ArtifactManager.$anonfun$withResources$1(ArtifactManager.scala:112)\r\n\tat org.apache.spark.sql.artifact.ArtifactManager.withClassLoaderIfNeeded(ArtifactManager.scala:106)\r\n\tat org.apache.spark.sql.artifact.ArtifactManager.withResources(ArtifactManager.scala:111)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$6(SQLExecution.scala:124)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:291)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$1(SQLExecution.scala:123)\r\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:804)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId0(SQLExecution.scala:77)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:233)\r\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$1(QueryExecution.scala:155)\r\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:654)\r\n\tat org.apache.spark.sql.execution.QueryExecution.org$apache$spark$sql$execution$QueryExecution$$eagerlyExecute$1(QueryExecution.scala:154)\r\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$3.applyOrElse(QueryExecution.scala:169)\r\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$3.applyOrElse(QueryExecution.scala:164)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:470)\r\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:86)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:470)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:37)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:360)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:356)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:37)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:37)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:446)\r\n\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:164)\r\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyCommandExecuted$1(QueryExecution.scala:126)\r\n\tat scala.util.Try$.apply(Try.scala:217)\r\n\tat org.apache.spark.util.Utils$.doTryWithCallerStacktrace(Utils.scala:1378)\r\n\tat org.apache.spark.util.Utils$.getTryWithCallerStacktrace(Utils.scala:1439)\r\n\tat org.apache.spark.util.LazyTry.get(LazyTry.scala:58)\r\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:131)\r\n\tat org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:192)\r\n\tat org.apache.spark.sql.classic.DataFrameWriter.runCommand(DataFrameWriter.scala:622)\r\n\tat org.apache.spark.sql.classic.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:273)\r\n\tat org.apache.spark.sql.classic.DataFrameWriter.saveInternal(DataFrameWriter.scala:241)\r\n\tat org.apache.spark.sql.classic.DataFrameWriter.save(DataFrameWriter.scala:118)\r\n\tat org.apache.spark.sql.DataFrameWriter.csv(DataFrameWriter.scala:426)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\r\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:569)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:184)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:108)\r\n\tat java.base/java.lang.Thread.run(Thread.java:840)\r\n\tSuppressed: org.apache.spark.util.Utils$OriginalTryStackTraceException: Full stacktrace of original doTryWithCallerStacktrace caller\r\n\t\tat org.apache.hadoop.util.Shell.getWinUtilsPath(Shell.java:789)\r\n\t\tat org.apache.hadoop.util.Shell.getSetPermissionCommand(Shell.java:298)\r\n\t\tat org.apache.hadoop.util.Shell.getSetPermissionCommand(Shell.java:314)\r\n\t\tat org.apache.hadoop.fs.RawLocalFileSystem.setPermission(RawLocalFileSystem.java:1116)\r\n\t\tat org.apache.hadoop.fs.RawLocalFileSystem.mkOneDirWithMode(RawLocalFileSystem.java:798)\r\n\t\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirsWithOptionalPermission(RawLocalFileSystem.java:838)\r\n\t\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirs(RawLocalFileSystem.java:810)\r\n\t\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirsWithOptionalPermission(RawLocalFileSystem.java:837)\r\n\t\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirs(RawLocalFileSystem.java:810)\r\n\t\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirsWithOptionalPermission(RawLocalFileSystem.java:837)\r\n\t\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirs(RawLocalFileSystem.java:810)\r\n\t\tat org.apache.hadoop.fs.ChecksumFileSystem.mkdirs(ChecksumFileSystem.java:988)\r\n\t\tat org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.setupJob(FileOutputCommitter.java:356)\r\n\t\tat org.apache.spark.internal.io.HadoopMapReduceCommitProtocol.setupJob(HadoopMapReduceCommitProtocol.scala:190)\r\n\t\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.writeAndCommit(FileFormatWriter.scala:268)\r\n\t\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeWrite(FileFormatWriter.scala:306)\r\n\t\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:189)\r\n\t\tat org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:195)\r\n\t\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:117)\r\n\t\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:115)\r\n\t\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.executeCollect(commands.scala:129)\r\n\t\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$2(QueryExecution.scala:155)\r\n\t\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$8(SQLExecution.scala:162)\r\n\t\tat org.apache.spark.sql.execution.SQLExecution$.withSessionTagsApplied(SQLExecution.scala:268)\r\n\t\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$7(SQLExecution.scala:124)\r\n\t\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)\r\n\t\tat org.apache.spark.sql.artifact.ArtifactManager.$anonfun$withResources$1(ArtifactManager.scala:112)\r\n\t\tat org.apache.spark.sql.artifact.ArtifactManager.withClassLoaderIfNeeded(ArtifactManager.scala:106)\r\n\t\tat org.apache.spark.sql.artifact.ArtifactManager.withResources(ArtifactManager.scala:111)\r\n\t\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$6(SQLExecution.scala:124)\r\n\t\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:291)\r\n\t\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$1(SQLExecution.scala:123)\r\n\t\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:804)\r\n\t\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId0(SQLExecution.scala:77)\r\n\t\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:233)\r\n\t\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$1(QueryExecution.scala:155)\r\n\t\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:654)\r\n\t\tat org.apache.spark.sql.execution.QueryExecution.org$apache$spark$sql$execution$QueryExecution$$eagerlyExecute$1(QueryExecution.scala:154)\r\n\t\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$3.applyOrElse(QueryExecution.scala:169)\r\n\t\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$3.applyOrElse(QueryExecution.scala:164)\r\n\t\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:470)\r\n\t\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:86)\r\n\t\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:470)\r\n\t\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:37)\r\n\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:360)\r\n\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:356)\r\n\t\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:37)\r\n\t\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:37)\r\n\t\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:446)\r\n\t\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:164)\r\n\t\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyCommandExecuted$1(QueryExecution.scala:126)\r\n\t\tat scala.util.Try$.apply(Try.scala:217)\r\n\t\tat org.apache.spark.util.Utils$.doTryWithCallerStacktrace(Utils.scala:1378)\r\n\t\tat org.apache.spark.util.LazyTry.tryT$lzycompute(LazyTry.scala:46)\r\n\t\tat org.apache.spark.util.LazyTry.tryT(LazyTry.scala:46)\r\n\t\t... 20 more\r\nCaused by: java.io.FileNotFoundException: java.io.FileNotFoundException: HADOOP_HOME and hadoop.home.dir are unset. -see https://cwiki.apache.org/confluence/display/HADOOP2/WindowsProblems\r\n\tat org.apache.hadoop.util.Shell.fileNotFoundException(Shell.java:601)\r\n\tat org.apache.hadoop.util.Shell.getHadoopHomeDir(Shell.java:622)\r\n\tat org.apache.hadoop.util.Shell.getQualifiedBin(Shell.java:645)\r\n\tat org.apache.hadoop.util.Shell.<clinit>(Shell.java:742)\r\n\tat org.apache.hadoop.util.StringUtils.<clinit>(StringUtils.java:80)\r\n\tat org.apache.hadoop.conf.Configuration.getTimeDurationHelper(Configuration.java:1954)\r\n\tat org.apache.hadoop.conf.Configuration.getTimeDuration(Configuration.java:1912)\r\n\tat org.apache.hadoop.conf.Configuration.getTimeDuration(Configuration.java:1885)\r\n\tat org.apache.hadoop.util.ShutdownHookManager.getShutdownTimeout(ShutdownHookManager.java:183)\r\n\tat org.apache.hadoop.util.ShutdownHookManager$HookEntry.<init>(ShutdownHookManager.java:207)\r\n\tat org.apache.hadoop.util.ShutdownHookManager.addShutdownHook(ShutdownHookManager.java:304)\r\n\tat org.apache.spark.util.SparkShutdownHookManager.$anonfun$install$1(ShutdownHookManager.scala:194)\r\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\r\n\tat scala.Option.fold(Option.scala:263)\r\n\tat org.apache.spark.util.SparkShutdownHookManager.install(ShutdownHookManager.scala:195)\r\n\tat org.apache.spark.util.ShutdownHookManager$.shutdownHooks$lzycompute(ShutdownHookManager.scala:55)\r\n\tat org.apache.spark.util.ShutdownHookManager$.shutdownHooks(ShutdownHookManager.scala:53)\r\n\tat org.apache.spark.util.ShutdownHookManager$.addShutdownHook(ShutdownHookManager.scala:159)\r\n\tat org.apache.spark.util.ShutdownHookManager$.<clinit>(ShutdownHookManager.scala:63)\r\n\tat org.apache.spark.util.Utils$.createTempDir(Utils.scala:250)\r\n\tat org.apache.spark.util.SparkFileUtils.createTempDir(SparkFileUtils.scala:103)\r\n\tat org.apache.spark.util.SparkFileUtils.createTempDir$(SparkFileUtils.scala:102)\r\n\tat org.apache.spark.util.Utils$.createTempDir(Utils.scala:99)\r\n\tat org.apache.spark.deploy.SparkSubmit.prepareSubmitEnvironment(SparkSubmit.scala:379)\r\n\tat org.apache.spark.deploy.SparkSubmit.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:961)\r\n\tat org.apache.spark.deploy.SparkSubmit.doRunMain$1(SparkSubmit.scala:204)\r\n\tat org.apache.spark.deploy.SparkSubmit.submit(SparkSubmit.scala:227)\r\n\tat org.apache.spark.deploy.SparkSubmit.doSubmit(SparkSubmit.scala:96)\r\n\tat org.apache.spark.deploy.SparkSubmit$$anon$2.doSubmit(SparkSubmit.scala:1132)\r\n\tat org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:1141)\r\n\tat org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)\r\nCaused by: java.io.FileNotFoundException: HADOOP_HOME and hadoop.home.dir are unset.\r\n\tat org.apache.hadoop.util.Shell.checkHadoopHomeInner(Shell.java:521)\r\n\tat org.apache.hadoop.util.Shell.checkHadoopHome(Shell.java:492)\r\n\tat org.apache.hadoop.util.Shell.<clinit>(Shell.java:569)\r\n\t... 27 more\r\n"
     ]
    }
   ],
   "source": [
    "# üíæ Sauvegarde des donn√©es ing√©r√©es au format CSV\n",
    "import os\n",
    "\n",
    "# Chemin absolu vers le dossier de sortie CSV\n",
    "output_dir = os.path.abspath(os.path.join(os.getcwd(), \"../data/step1_raw_csv\"))\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# √âcriture au format CSV (avec header)\n",
    "df_raw.write \\\n",
    ".option(\"header\", \"true\") \\\n",
    ".option(\"sep\", \";\")\\\n",
    ".mode(\"overwrite\") \\\n",
    ".csv(output_dir)\n",
    "\n",
    "print(f\"‚úÖ Ingestion CSV termin√©e dans : {output_dir}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b81373ff17b87fb",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# üíæ Sauvegarde au format Parquet\n",
    "print(\"üöÄ D√©but de la comparaison CSV vs Parquet\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Utilisation du chemin CSV existant et cr√©ation du chemin Parquet\n",
    "csv_output_dir = output_dir  # Utilise la variable existante pour le CSV\n",
    "parquet_output_dir = os.path.abspath(os.path.join(os.getcwd(), \"../data/step1_raw_parquet\"))\n",
    "\n",
    "# Cr√©ation du dossier Parquet (CSV d√©j√† cr√©√©)\n",
    "os.makedirs(parquet_output_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43aa7507f36bd735",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# üìä Comparatif des performances entre CSV et Parquet\n",
    "import time\n",
    "\n",
    "# ‚è±Ô∏è Mesure des performances d'√©criture\n",
    "\n",
    "print(\"\\nüìù Phase 1: Mesure des performances d'√©criture\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "# ‚úçÔ∏è √âcriture CSV (d√©j√† effectu√©e, on mesure juste le temps pour la comparaison)\n",
    "print(\"‚è≥ Re-mesure du temps d'√©criture CSV pour comparaison...\")\n",
    "start_time = time.time()\n",
    "\n",
    "df_raw.write \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .option(\"sep\", \";\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .csv(csv_output_dir)\n",
    "\n",
    "csv_write_time = time.time() - start_time\n",
    "print(f\"‚úÖ √âcriture CSV termin√©e en {csv_write_time:.2f} secondes\")\n",
    "\n",
    "# ‚úçÔ∏è √âcriture Parquet\n",
    "print(\"‚è≥ √âcriture au format Parquet...\")\n",
    "start_time = time.time()\n",
    "\n",
    "df_raw.write \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .parquet(parquet_output_dir)\n",
    "\n",
    "parquet_write_time = time.time() - start_time\n",
    "print(f\"‚úÖ √âcriture Parquet termin√©e en {parquet_write_time:.2f} secondes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb00547d71771692",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# üìñ Mesure des performances de lecture\n",
    "\n",
    "print(\"\\nüìñ Phase 2: Mesure des performances de lecture\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "# üìö Lecture CSV\n",
    "print(\"‚è≥ Lecture du format CSV...\")\n",
    "start_time = time.time()\n",
    "\n",
    "df_csv = spark.read \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .option(\"sep\", \";\") \\\n",
    "    .option(\"inferSchema\", \"true\") \\\n",
    "    .csv(csv_output_dir)\n",
    "\n",
    "# Action pour d√©clencher la lecture\n",
    "csv_count = df_csv.count()\n",
    "csv_read_time = time.time() - start_time\n",
    "print(f\"‚úÖ Lecture CSV termin√©e en {csv_read_time:.2f} secondes ({csv_count:,} lignes)\")\n",
    "\n",
    "# üìö Lecture Parquet\n",
    "print(\"‚è≥ Lecture du format Parquet...\")\n",
    "start_time = time.time()\n",
    "\n",
    "df_parquet = spark.read.parquet(parquet_output_dir)\n",
    "\n",
    "# Action pour d√©clencher la lecture\n",
    "parquet_count = df_parquet.count()\n",
    "parquet_read_time = time.time() - start_time\n",
    "print(f\"‚úÖ Lecture Parquet termin√©e en {parquet_read_time:.2f} secondes ({parquet_count:,} lignes)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f42aa7bca88553a1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-20T09:53:00.131759600Z",
     "start_time": "2025-06-20T09:52:59.812712100Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# üìè Mesure de la taille des fichiers\n",
    "\n",
    "print(\"\\nüìè PHASE 3: Comparaison de la taille des fichiers\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "def get_directory_size(path):\n",
    "    \"\"\"Calcule la taille totale d'un dossier en octets\"\"\"\n",
    "    total_size = 0\n",
    "    for dirpath, dirnames, filenames in os.walk(path):\n",
    "        for filename in filenames:\n",
    "            filepath = os.path.join(dirpath, filename)\n",
    "            if os.path.exists(filepath):\n",
    "                total_size += os.path.getsize(filepath)\n",
    "    return total_size\n",
    "\n",
    "def format_size(size_bytes):\n",
    "    \"\"\"Formate la taille en unit√©s lisibles\"\"\"\n",
    "    if size_bytes == 0:\n",
    "        return \"0 B\"\n",
    "    \n",
    "    units = ['B', 'KB', 'MB', 'GB', 'TB']\n",
    "    i = 0\n",
    "    while size_bytes >= 1024 and i < len(units) - 1:\n",
    "        size_bytes /= 1024\n",
    "        i += 1\n",
    "    \n",
    "    return f\"{size_bytes:.2f} {units[i]}\"\n",
    "\n",
    "# Calcul des tailles\n",
    "csv_size = get_directory_size(csv_output_dir)\n",
    "parquet_size = get_directory_size(parquet_output_dir)\n",
    "\n",
    "print(f\"üìÅ Taille CSV: {format_size(csv_size)}\")\n",
    "print(f\"üìÅ Taille Parquet: {format_size(parquet_size)}\")\n",
    "\n",
    "# Calcul du ratio de compression\n",
    "if csv_size > 0:\n",
    "    compression_ratio = (csv_size - parquet_size) / csv_size * 100\n",
    "    print(f\"üìä Compression: {compression_ratio:.1f}% (Parquet vs CSV)\")\n",
    "else:\n",
    "    compression_ratio = 0\n",
    "    print(\"‚ö†Ô∏è Impossible de calculer le ratio de compression car la taille du CSV est nulle.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b948bde29cbc9e21",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# üìä R√©sum√© des performances\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"üìä R√©sum√© des performances des formats CSV vs Parquet\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(f\"\"\"\n",
    "üî∏ √âCRITURE:\n",
    "   ‚Ä¢ CSV:     {csv_write_time:.2f}s\n",
    "   ‚Ä¢ Parquet: {parquet_write_time:.2f}s\n",
    "   ‚Ä¢ Gain:    {((csv_write_time - parquet_write_time) / csv_write_time * 100):+.1f}%\n",
    "\n",
    "üî∏ LECTURE:\n",
    "   ‚Ä¢ CSV:     {csv_read_time:.2f}s\n",
    "   ‚Ä¢ Parquet: {parquet_read_time:.2f}s\n",
    "   ‚Ä¢ Gain:    {((csv_read_time - parquet_read_time) / csv_read_time * 100):+.1f}%\n",
    "\n",
    "üî∏ TAILLE:\n",
    "   ‚Ä¢ CSV:     {format_size(csv_size)}\n",
    "   ‚Ä¢ Parquet: {format_size(parquet_size)}\n",
    "   ‚Ä¢ Gain:    {compression_ratio:.1f}%\n",
    "\n",
    "üî∏ PERFORMANCE GLOBALE:\n",
    "   ‚Ä¢ Temps total CSV:     {csv_write_time + csv_read_time:.2f}s\n",
    "   ‚Ä¢ Temps total Parquet: {parquet_write_time + parquet_read_time:.2f}s\n",
    "   ‚Ä¢ Gain total:          {((csv_write_time + csv_read_time - parquet_write_time - parquet_read_time) / (csv_write_time + csv_read_time) * 100):+.1f}%\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e549c7138a7ffb0",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# üìù Recommandations et conclusion\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"üìù Recommandations\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "recommendations = []\n",
    "\n",
    "if parquet_write_time < csv_write_time:\n",
    "    recommendations.append(\"‚úÖ Parquet est plus rapide en √©criture\")\n",
    "else:\n",
    "    recommendations.append(\"‚ö†Ô∏è CSV est plus rapide en √©criture\")\n",
    "\n",
    "if parquet_read_time < csv_read_time:\n",
    "    recommendations.append(\"‚úÖ Parquet est plus rapide en lecture\")\n",
    "else:\n",
    "    recommendations.append(\"‚ö†Ô∏è CSV est plus rapide en lecture\")\n",
    "\n",
    "if parquet_size < csv_size:\n",
    "    recommendations.append(\"‚úÖ Parquet occupe moins d'espace disque\")\n",
    "else:\n",
    "    recommendations.append(\"‚ö†Ô∏è CSV occupe moins d'espace disque\")\n",
    "\n",
    "# if parquet_query_time < csv_query_time:\n",
    "    # recommendations.append(\"‚úÖ Parquet est plus performant pour les requ√™tes\")\n",
    "# else:\n",
    "    #recommendations.append(\"‚ö†Ô∏è CSV est plus performant pour les requ√™tes\")\n",
    "\n",
    "for rec in recommendations:\n",
    "    print(f\"  {rec}\")\n",
    "\n",
    "print(f\"\\nüéØ Conclusion: {'Parquet' if len([r for r in recommendations if 'Parquet' in r and '‚úÖ' in r]) >= 2 else 'CSV'} semble √™tre le meilleur choix pour ce dataset.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e94102ef94b79c00",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# üìä Visualisations des performances avec matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Configuration matplotlib pour de beaux graphiques\n",
    "plt.style.use('default')\n",
    "plt.rcParams['figure.figsize'] = (15, 10)\n",
    "plt.rcParams['font.size'] = 11\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"G√©n√©ration des visualisations\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "def format_size_mb(size_bytes):\n",
    "    \"\"\"Formate la taille en MB\"\"\"\n",
    "    return f'{size_bytes / (1024*1024):.1f} MB'\n",
    "\n",
    "def create_performance_dashboard():\n",
    "    \"\"\"Cr√©e un dashboard complet des performances\"\"\"\n",
    "\n",
    "    # Cr√©ation de la figure avec 6 sous-graphiques\n",
    "    fig, ((ax1, ax2, ax3), (ax4, ax5, ax6)) = plt.subplots(2, 3, figsize=(18, 12))\n",
    "\n",
    "    # === 1. Temps d'op√©ration (barres) ===\n",
    "    operations = ['√âcriture', 'Lecture']\n",
    "    csv_times = [csv_write_time, csv_read_time]\n",
    "    parquet_times = [parquet_write_time, parquet_read_time]\n",
    "\n",
    "    x = np.arange(len(operations))\n",
    "    width = 0.35\n",
    "\n",
    "    bars1 = ax1.bar(x - width/2, csv_times, width, label='CSV', color='orange', alpha=0.7)\n",
    "    bars2 = ax1.bar(x + width/2, parquet_times, width, label='Parquet', color='green', alpha=0.7)\n",
    "\n",
    "    ax1.set_xlabel('Op√©rations')\n",
    "    ax1.set_ylabel('Temps (secondes)')\n",
    "    ax1.set_title('Temps d\\'op√©ration')\n",
    "    ax1.set_xticks(x)\n",
    "    ax1.set_xticklabels(operations)\n",
    "    ax1.legend()\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "\n",
    "    for bar in bars1:\n",
    "        height = bar.get_height()\n",
    "        ax1.text(bar.get_x() + bar.get_width()/2., height + 2,\n",
    "                 f'{height:.1f}s', ha='center', va='bottom', fontweight='bold')\n",
    "\n",
    "    for bar in bars2:\n",
    "        height = bar.get_height()\n",
    "        ax1.text(bar.get_x() + bar.get_width()/2., height + 2,\n",
    "                 f'{height:.1f}s', ha='center', va='bottom', fontweight='bold')\n",
    "\n",
    "    # === 2. R√©partition de l'espace disque (secteurs) ===\n",
    "    sizes = [csv_size, parquet_size]\n",
    "    labels = ['CSV', 'Parquet']\n",
    "    colors = ['orange', 'green']\n",
    "\n",
    "    wedges, texts, autotexts = ax2.pie(sizes, labels=labels, colors=colors,\n",
    "                                       autopct='%1.1f%%', startangle=90)\n",
    "    ax2.set_title('Espace disque utilis√©')\n",
    "\n",
    "    legend_labels = [f'{label}: {format_size_mb(size)}' for label, size in zip(labels, sizes)]\n",
    "    ax2.legend(wedges, legend_labels, loc=\"center left\", bbox_to_anchor=(1, 0, 0.5, 1))\n",
    "\n",
    "    # === 3. Gains de performance ===\n",
    "    write_gain = ((csv_write_time - parquet_write_time) / csv_write_time * 100)\n",
    "    read_gain = ((csv_read_time - parquet_read_time) / csv_read_time * 100)\n",
    "    size_gain = ((csv_size - parquet_size) / csv_size * 100)\n",
    "    total_gain = (((csv_write_time + csv_read_time) - (parquet_write_time + parquet_read_time)) /\n",
    "                  (csv_write_time + csv_read_time) * 100)\n",
    "\n",
    "    metrics = ['√âcriture', 'Lecture', 'Taille', 'Total']\n",
    "    gains = [write_gain, read_gain, size_gain, total_gain]\n",
    "    colors_gain = ['green' if g > 0 else 'red' for g in gains]\n",
    "\n",
    "    bars = ax3.bar(metrics, gains, color=colors_gain, alpha=0.7)\n",
    "    ax3.set_ylabel('Gain (%)')\n",
    "    ax3.set_title('Gains Parquet vs CSV')\n",
    "    ax3.axhline(y=0, color='black', linestyle='-', linewidth=0.8)\n",
    "    ax3.grid(True, alpha=0.3)\n",
    "\n",
    "    for bar, gain in zip(bars, gains):\n",
    "        height = bar.get_height()\n",
    "        ax3.text(bar.get_x() + bar.get_width()/2.,\n",
    "                 height + (1 if height > 0 else -2),\n",
    "                 f'{gain:+.1f}%', ha='center',\n",
    "                 va='bottom' if height > 0 else 'top',\n",
    "                 fontweight='bold')\n",
    "\n",
    "    # === 4. Comparaison temps total ===\n",
    "    total_times = [csv_write_time + csv_read_time, parquet_write_time + parquet_read_time]\n",
    "    formats = ['CSV', 'Parquet']\n",
    "    colors_total = ['orange', 'green']\n",
    "\n",
    "    bars = ax4.bar(formats, total_times, color=colors_total, alpha=0.7)\n",
    "    ax4.set_ylabel('Temps total (secondes)')\n",
    "    ax4.set_title('Temps total (√âcriture + Lecture)')\n",
    "    ax4.grid(True, alpha=0.3)\n",
    "\n",
    "    for bar, time in zip(bars, total_times):\n",
    "        height = bar.get_height()\n",
    "        ax4.text(bar.get_x() + bar.get_width()/2., height + 5,\n",
    "                 f'{height:.1f}s', ha='center', va='bottom', fontweight='bold')\n",
    "\n",
    "    # === 5. Efficacit√© relative ===\n",
    "    csv_score = 1000 / (csv_write_time + csv_read_time + csv_size/(1024**2))\n",
    "    parquet_score = 1000 / (parquet_write_time + parquet_read_time + parquet_size/(1024**2))\n",
    "\n",
    "    scores = [csv_score, parquet_score]\n",
    "\n",
    "    bars = ax5.bar(formats, scores, color=colors_total, alpha=0.7)\n",
    "    ax5.set_ylabel('Score d\\'efficacit√©')\n",
    "    ax5.set_title('Score d\\'efficacit√© global')\n",
    "    ax5.grid(True, alpha=0.3)\n",
    "\n",
    "    for bar, score in zip(bars, scores):\n",
    "        height = bar.get_height()\n",
    "        ax5.text(bar.get_x() + bar.get_width()/2., height + 0.1,\n",
    "                 f'{score:.1f}', ha='center', va='bottom', fontweight='bold')\n",
    "\n",
    "    # === 6. R√©sum√© textuel ===\n",
    "    ax6.axis('off')\n",
    "\n",
    "    summary = f\"\"\"R√âSUM√â EX√âCUTIF\n",
    "    \n",
    "TEMPS (secondes)\n",
    "√âcriture: CSV {csv_write_time:.1f}s vs Parquet {parquet_write_time:.1f}s\n",
    "Lecture:  CSV {csv_read_time:.1f}s vs Parquet {parquet_read_time:.1f}s\n",
    "\n",
    "ESPACE DISQUE\n",
    "CSV:     {format_size_mb(csv_size)}\n",
    "Parquet: {format_size_mb(parquet_size)}\n",
    "\n",
    "GAINS PARQUET\n",
    "Temps total: {total_gain:+.1f}%\n",
    "Espace:      {size_gain:+.1f}%\n",
    "\n",
    "VERDICT\n",
    "Recommandation: {'PARQUET' if total_gain > 0 and size_gain > 0 else 'CSV'}\n",
    "\n",
    "Parquet excelle en:\n",
    "‚Ä¢ Compression des donn√©es\n",
    "‚Ä¢ Vitesse de lecture\n",
    "‚Ä¢ Performance analytique\"\"\"\n",
    "\n",
    "    ax6.text(0.05, 0.95, summary, transform=ax6.transAxes, fontsize=10,\n",
    "             verticalalignment='top', fontfamily='monospace',\n",
    "             bbox=dict(boxstyle=\"round,pad=0.5\", facecolor=\"lightblue\", alpha=0.8))\n",
    "\n",
    "    plt.suptitle('Dashboard Performance: CSV vs Parquet\\nDataset OpenFoodFacts (3.9M lignes)',\n",
    "                 fontsize=14, fontweight='bold', y=0.98)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.subplots_adjust(top=0.93)\n",
    "    plt.show()\n",
    "\n",
    "# G√©n√©ration du dashboard\n",
    "create_performance_dashboard()\n",
    "\n",
    "print(\"Visualisations g√©n√©r√©es avec succ√®s!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
