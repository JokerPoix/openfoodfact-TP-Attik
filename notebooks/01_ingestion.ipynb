{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6ec0b7d9",
   "metadata": {},
   "source": [
    "# ü•£ 01 - Ingestion des donn√©es OpenFoodFacts\n",
    "Ce notebook a pour objectif de lire les donn√©es OpenFoodFacts brutes au format `.csv.gz`, de les analyser rapidement et de les convertir en format `Parquet` pour les √©tapes suivantes du pipeline."
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### üì¶ Import des biblioth√®ques essentielles\n",
    "\n",
    "**Biblioth√®ques utilis√©es :**\n",
    "- `pyspark.sql.SparkSession` : Point d'entr√©e principal pour la programmation Spark avec l'API DataFrame\n",
    "- `pyspark.sql.functions.col` : Fonction pour r√©f√©rencer les colonnes dans les transformations\n",
    "- `os` : Module pour les op√©rations syst√®me (gestion des chemins, cr√©ation de dossiers)\n",
    "\n",
    "Ces imports permettent de configurer l'environnement Spark n√©cessaire au traitement de donn√©es volumineuses."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7f575120477eefc1"
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8240b1fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üì¶ Imports principaux\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### üöÄ Initialisation de la SparkSession\n",
    "\n",
    "**Fonction :** Cr√©e une instance SparkSession qui est le point d'entr√©e pour toutes les fonctionnalit√©s Spark.\n",
    "\n",
    "**Param√®tres :**\n",
    "- `.appName(\"OpenFoodFacts Ingestion\")` : D√©finit un nom d'application pour identifier les jobs Spark\n",
    "- `.getOrCreate()` : R√©cup√®re une session existante ou en cr√©e une nouvelle si n√©cessaire\n",
    "\n",
    "La SparkSession est essentielle pour :\n",
    "- Lire des donn√©es depuis diverses sources\n",
    "- Cr√©er des DataFrames\n",
    "- Acc√©der aux fonctionnalit√©s SQL de Spark"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f042b530b9934455"
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6b42bfea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üöÄ Cr√©ation de la SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"OpenFoodFacts Ingestion\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### üì• Lecture du fichier OpenFoodFacts\n",
    "\n",
    "**Fonction :** Charge le fichier CSV compress√© (gzip) contenant les donn√©es OpenFoodFacts.\n",
    "\n",
    "**Options de lecture :**\n",
    "- `.option(\"header\", True)` : Indique que la premi√®re ligne contient les noms de colonnes\n",
    "- `.option(\"sep\", \"\\t\")` : Sp√©cifie que le s√©parateur est une tabulation (format TSV)\n",
    "- `.option(\"inferSchema\", True)` : Demande √† Spark de d√©duire automatiquement les types de donn√©es\n",
    "- `.csv(input_path)` : Lit le fichier au format CSV/TSV\n",
    "\n",
    "**Optimisation :**\n",
    "- `.cache()` : Met en cache le DataFrame en m√©moire pour acc√©l√©rer les op√©rations ult√©rieures\n",
    "- `.printSchema()` : Affiche la structure d√©taill√©e du DataFrame (colonnes et types)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "58183993ea9f448d"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24d92544",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üì• Lecture du fichier .csv.gz (format TSV)\n",
    "input_path = \"../data/en.openfoodfacts.org.products.csv.gz\"\n",
    "\n",
    "df_raw = spark.read.option(\"header\", True) \\\n",
    "                   .option(\"sep\", \"\\t\") \\\n",
    "                   .option(\"inferSchema\", True) \\\n",
    "                   .csv(input_path)\n",
    "\n",
    "df_raw.cache()\n",
    "df_raw.printSchema()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### üî¢ Analyse des dimensions du dataframe\n",
    "\n",
    "**Fonction :** Calcule et affiche les dimensions du DataFrame charg√©.\n",
    "\n",
    "**M√©triques calcul√©es :**\n",
    "- `df_raw.count()` : Compte le nombre total de lignes (produits) dans le dataset\n",
    "- `len(df_raw.columns)` : D√©termine le nombre de colonnes (attributs) par produit\n",
    "\n",
    "Ces informations permettent de comprendre la volum√©trie des donn√©es :\n",
    "- Volume total de donn√©es √† traiter\n",
    "- Complexit√© de la structure (nombre d'attributs par produit)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "26c5d0df50c49d96"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ad32c0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üî¢ Dimensions du DataFrame\n",
    "n_rows = df_raw.count()\n",
    "n_cols = len(df_raw.columns)\n",
    "print(f\"Nombre de lignes: {n_rows:,}\")\n",
    "print(f\"Nombre de colonnes: {n_cols}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### üìã Liste d√©taill√©e des colonnes\n",
    "\n",
    "**Fonction :** Affiche la liste compl√®te des colonnes avec num√©rotation.\n",
    "\n",
    "**Utilit√© :**\n",
    "- Permet d'identifier rapidement les attributs disponibles\n",
    "- Facilite la s√©lection future de colonnes pertinentes\n",
    "- Aide √† comprendre la richesse des donn√©es OpenFoodFacts\n",
    "\n",
    "La num√©rotation (format `01. nom_colonne`) am√©liore la lisibilit√© pour les datasets avec de nombreuses colonnes."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1ffa1b78d418e53e"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a145c17c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üìã Affichage des noms de colonnes\n",
    "print(\"\\nüßæ Liste des colonnes :\")\n",
    "for i, col_name in enumerate(df_raw.columns, start=1):\n",
    "    print(f\"{i:02d}. {col_name}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### üíæ √âcriture des donn√©es au format CSV\n",
    "\n",
    "**Fonction :** Sauvegarde le DataFrame au format CSV pour √©tablir une base de comparaison.\n",
    "\n",
    "**Processus :**\n",
    "1. Cr√©ation du dossier de sortie avec `os.makedirs()`\n",
    "2. Mesure du temps d'√©criture avec `time.time()`\n",
    "3. √âcriture des donn√©es avec options :\n",
    "   - `.option(\"header\", \"true\")` : Inclut les en-t√™tes de colonnes\n",
    "   - `.option(\"sep\", \";\")` : Utilise le point-virgule comme s√©parateur\n",
    "   - `.mode(\"overwrite\")` : √âcrase les fichiers existants\n",
    "\n",
    "**Objectif :** √âtablir une r√©f√©rence de performance pour comparer avec Parquet par la suite."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "841b2b2f15b84d3c"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d021f69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üíæ Sauvegarde des donn√©es ing√©r√©es au format CSV\n",
    "import os\n",
    "import time\n",
    "\n",
    "# ‚è±Ô∏è Mesure des performances d'√©criture\n",
    "\n",
    "print(\"\\nüìù Phase 1: Mesure des performances d'√©criture\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "# ‚úçÔ∏è Mesure du temps d'√©criture au format CSV\n",
    "print(\"‚è≥ Mesure du temps d'√©criture CSV pour comparaison...\")\n",
    "start_time = time.time()\n",
    "\n",
    "# Chemin absolu vers le dossier de sortie CSV\n",
    "output_dir = os.path.abspath(os.path.join(os.getcwd(), \"../data/step1_raw_csv\"))\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# √âcriture au format CSV (avec header)\n",
    "df_raw.write \\\n",
    ".option(\"header\", \"true\") \\\n",
    ".option(\"sep\", \";\")\\\n",
    ".mode(\"overwrite\") \\\n",
    ".csv(output_dir)\n",
    "\n",
    "csv_write_time = time.time() - start_time\n",
    "print(f\"‚úÖ √âcriture CSV termin√©e en {csv_write_time:.2f} secondes\")\n",
    "# Affichage du chemin de sortie\n",
    "print(f\"‚úÖ Ingestion CSV termin√©e dans : {output_dir}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### üöÄ Configuration pour la comparaison CSV vs Parquet\n",
    "\n",
    "**Fonction :** Pr√©pare l'environnement pour la comparaison des formats.\n",
    "\n",
    "**Actions :**\n",
    "- D√©finition des chemins de sortie pour Parquet\n",
    "- Cr√©ation des dossiers n√©cessaires\n",
    "- Utilisation de chemins absolus pour √©viter les probl√®mes de localisation."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "978a5e5659951b2e"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b81373ff17b87fb",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# üíæ Sauvegarde au format Parquet\n",
    "print(\"üöÄ D√©but de la comparaison CSV vs Parquet\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Cr√©ation du chemin Parquet\n",
    "\n",
    "parquet_output_dir = os.path.abspath(os.path.join(os.getcwd(), \"../data/step1_raw_parquet\"))\n",
    "\n",
    "# Cr√©ation du dossier Parquet (CSV d√©j√† cr√©√©)\n",
    "os.makedirs(parquet_output_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Mesure :** Le temps d'√©criture est chronom√©tr√© pour comparaison avec CSV."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ebf42608c0c97a91"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43aa7507f36bd735",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# üìä Comparatif des performances entre CSV et Parquet\n",
    "\n",
    "# ‚úçÔ∏è √âcriture Parquet\n",
    "print(\"‚è≥ √âcriture au format Parquet...\")\n",
    "start_time = time.time()\n",
    "\n",
    "df_raw.write \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .parquet(parquet_output_dir)\n",
    "\n",
    "parquet_write_time = time.time() - start_time\n",
    "print(f\"‚úÖ √âcriture Parquet termin√©e en {parquet_write_time:.2f} secondes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### üìñ Comparaison des performances de lecture\n",
    "\n",
    "**Fonction :** Mesure et compare les temps de lecture entre CSV et Parquet.\n",
    "\n",
    "**Process pour chaque format :**\n",
    "1. Lecture du fichier avec les options appropri√©es\n",
    "2. Ex√©cution d'une action (`.count()`) pour forcer la lecture effective\n",
    "3. Mesure du temps total de l'op√©ration\n",
    "\n",
    "**Importance : La vitesse de lecture est importante pour :**\n",
    "- Les analyses it√©ratives\n",
    "- Les transformations complexes"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a1c63a897a9ff193"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb00547d71771692",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# üìñ Mesure des performances de lecture\n",
    "\n",
    "print(\"\\nüìñ Phase 2: Mesure des performances de lecture\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "# üìö Lecture CSV\n",
    "print(\"‚è≥ Lecture du format CSV...\")\n",
    "start_time = time.time()\n",
    "\n",
    "df_csv = spark.read \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .option(\"sep\", \";\") \\\n",
    "    .option(\"inferSchema\", \"true\") \\\n",
    "    .csv(output_dir)\n",
    "\n",
    "# Action pour d√©clencher la lecture\n",
    "csv_count = df_csv.count()\n",
    "csv_read_time = time.time() - start_time\n",
    "print(f\"‚úÖ Lecture CSV termin√©e en {csv_read_time:.2f} secondes ({csv_count:,} lignes)\")\n",
    "\n",
    "# üìö Lecture Parquet\n",
    "print(\"‚è≥ Lecture du format Parquet...\")\n",
    "start_time = time.time()\n",
    "\n",
    "df_parquet = spark.read.parquet(parquet_output_dir)\n",
    "\n",
    "# Action pour d√©clencher la lecture\n",
    "parquet_count = df_parquet.count()\n",
    "parquet_read_time = time.time() - start_time\n",
    "print(f\"‚úÖ Lecture Parquet termin√©e en {parquet_read_time:.2f} secondes ({parquet_count:,} lignes)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### üìè Comparaison de l'utilisation de l'espace disque\n",
    "\n",
    "**Fonctions helper :**\n",
    "- `get_directory_size()` : Calcule r√©cursivement la taille totale d'un dossier\n",
    "- `format_size()` : Convertit les octets en unit√©s lisibles (KB, MB, GB)\n",
    "\n",
    "**M√©triques calcul√©es :**\n",
    "- Taille totale de chaque format\n",
    "- Ratio de compression Parquet vs CSV\n",
    "- √âconomie d'espace en pourcentage\n",
    "\n",
    "**Cette analyse est importante pour :**\n",
    "- Estimer les co√ªts de stockage\n",
    "- Planifier l'infrastructure\n",
    "- Optimiser les pipelines de donn√©es"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "cc7adad9905b5480"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f42aa7bca88553a1",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# üìè Mesure de la taille des fichiers\n",
    "\n",
    "print(\"\\nüìè PHASE 3: Comparaison de la taille des fichiers\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "def get_directory_size(path):\n",
    "    \"\"\"Calcule la taille totale d'un dossier en octets\"\"\"\n",
    "    total_size = 0\n",
    "    for dirpath, dirnames, filenames in os.walk(path):\n",
    "        for filename in filenames:\n",
    "            filepath = os.path.join(dirpath, filename)\n",
    "            if os.path.exists(filepath):\n",
    "                total_size += os.path.getsize(filepath)\n",
    "    return total_size\n",
    "\n",
    "def format_size(size_bytes):\n",
    "    \"\"\"Formate la taille en unit√©s lisibles\"\"\"\n",
    "    if size_bytes == 0:\n",
    "        return \"0 B\"\n",
    "    \n",
    "    units = ['B', 'KB', 'MB', 'GB', 'TB']\n",
    "    i = 0\n",
    "    while size_bytes >= 1024 and i < len(units) - 1:\n",
    "        size_bytes /= 1024\n",
    "        i += 1\n",
    "    \n",
    "    return f\"{size_bytes:.2f} {units[i]}\"\n",
    "\n",
    "# Calcul des tailles\n",
    "csv_size = get_directory_size(output_dir)\n",
    "parquet_size = get_directory_size(parquet_output_dir)\n",
    "\n",
    "print(f\"üìÅ Taille CSV: {format_size(csv_size)}\")\n",
    "print(f\"üìÅ Taille Parquet: {format_size(parquet_size)}\")\n",
    "\n",
    "# Calcul du ratio de compression\n",
    "if csv_size > 0:\n",
    "    compression_ratio = (csv_size - parquet_size) / csv_size * 100\n",
    "    print(f\"üìä Compression: {compression_ratio:.1f}% (Parquet vs CSV)\")\n",
    "else:\n",
    "    compression_ratio = 0\n",
    "    print(\"‚ö†Ô∏è Impossible de calculer le ratio de compression car la taille du CSV est nulle.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### üìä Tableau de bord des performances\n",
    "\n",
    "**Fonction :** Synth√©tise toutes les m√©triques de performance dans un format structur√©.\n",
    "\n",
    "**M√©triques pr√©sent√©es :**\n",
    "- **√âcriture** : Temps et gain en pourcentage\n",
    "- **Lecture** : Temps et gain en pourcentage\n",
    "- **Taille** : Espace utilis√© et compression\n",
    "- **Performance globale** : Temps total et gain cumul√©\n",
    "\n",
    "Le format utilise des emojis et une mise en forme claire pour faciliter la lecture et l'interpr√©tation des r√©sultats."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c495df0ac22921e5"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b948bde29cbc9e21",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# üìä R√©sum√© des performances\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"üìä R√©sum√© des performances des formats CSV vs Parquet\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(f\"\"\"\n",
    "üî∏ √âCRITURE:\n",
    "   ‚Ä¢ CSV:     {csv_write_time:.2f}s\n",
    "   ‚Ä¢ Parquet: {parquet_write_time:.2f}s\n",
    "   ‚Ä¢ Gain:    {((csv_write_time - parquet_write_time) / csv_write_time * 100):+.1f}%\n",
    "\n",
    "üî∏ LECTURE:\n",
    "   ‚Ä¢ CSV:     {csv_read_time:.2f}s\n",
    "   ‚Ä¢ Parquet: {parquet_read_time:.2f}s\n",
    "   ‚Ä¢ Gain:    {((csv_read_time - parquet_read_time) / csv_read_time * 100):+.1f}%\n",
    "\n",
    "üî∏ TAILLE:\n",
    "   ‚Ä¢ CSV:     {format_size(csv_size)}\n",
    "   ‚Ä¢ Parquet: {format_size(parquet_size)}\n",
    "   ‚Ä¢ Gain:    {compression_ratio:.1f}%\n",
    "\n",
    "üî∏ PERFORMANCE GLOBALE:\n",
    "   ‚Ä¢ Temps total CSV:     {csv_write_time + csv_read_time:.2f}s\n",
    "   ‚Ä¢ Temps total Parquet: {parquet_write_time + parquet_read_time:.2f}s\n",
    "   ‚Ä¢ Gain total:          {((csv_write_time + csv_read_time - parquet_write_time - parquet_read_time) / (csv_write_time + csv_read_time) * 100):+.1f}%\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### üìù G√©n√©ration de recommandations\n",
    "\n",
    "**Fonction :** Analyse automatiquement les r√©sultats et g√©n√®re des recommandations.\n",
    "\n",
    "**Crit√®res d'√©valuation :**\n",
    "- Vitesse d'√©criture\n",
    "- Vitesse de lecture\n",
    "- Efficacit√© de compression\n",
    "- Performance globale\n",
    "\n",
    "**Logique :** \n",
    "- Compare chaque m√©trique entre CSV et Parquet\n",
    "- G√©n√®re des recommandations avec indicateurs visuels (‚úÖ/‚ö†Ô∏è)\n",
    "- Propose une conclusion bas√©e sur la majorit√© des crit√®res\n",
    "\n",
    "Cette approche objective aide √† la prise de d√©cision pour le choix du format."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3e87688358bd352"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e549c7138a7ffb0",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# üìù Recommandations et conclusion\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"üìù Recommandations\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "recommendations = []\n",
    "\n",
    "if parquet_write_time < csv_write_time:\n",
    "    recommendations.append(\"‚úÖ Parquet est plus rapide en √©criture\")\n",
    "else:\n",
    "    recommendations.append(\"‚ö†Ô∏è CSV est plus rapide en √©criture\")\n",
    "\n",
    "if parquet_read_time < csv_read_time:\n",
    "    recommendations.append(\"‚úÖ Parquet est plus rapide en lecture\")\n",
    "else:\n",
    "    recommendations.append(\"‚ö†Ô∏è CSV est plus rapide en lecture\")\n",
    "\n",
    "if parquet_size < csv_size:\n",
    "    recommendations.append(\"‚úÖ Parquet occupe moins d'espace disque\")\n",
    "else:\n",
    "    recommendations.append(\"‚ö†Ô∏è CSV occupe moins d'espace disque\")\n",
    "\n",
    "# if parquet_query_time < csv_query_time:\n",
    "    # recommendations.append(\"‚úÖ Parquet est plus performant pour les requ√™tes\")\n",
    "# else:\n",
    "    #recommendations.append(\"‚ö†Ô∏è CSV est plus performant pour les requ√™tes\")\n",
    "\n",
    "for rec in recommendations:\n",
    "    print(f\"  {rec}\")\n",
    "\n",
    "print(f\"\\nüéØ Conclusion: {'Parquet' if len([r for r in recommendations if 'Parquet' in r and '‚úÖ' in r]) >= 2 else 'CSV'} semble √™tre le meilleur choix pour ce dataset.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### üìä Dashboard visuel des performances\n",
    "\n",
    "**Fonction :** Cr√©e un dashboard complet avec 6 visualisations diff√©rentes.\n",
    "\n",
    "**Graphiques g√©n√©r√©s :**\n",
    "\n",
    "1. **Temps d'op√©ration (barres group√©es)** :\n",
    "   - Compare les temps d'√©criture et lecture\n",
    "   - Affiche les valeurs exactes sur chaque barre\n",
    "\n",
    "2. **R√©partition de l'espace disque (camembert)** :\n",
    "   - Visualise la proportion d'espace utilis√©\n",
    "   - Inclut les pourcentages et tailles absolues\n",
    "\n",
    "3. **Gains de performance (barres)** :\n",
    "   - Montre les gains de performance de Parquet par rapport √† CSV\n",
    "\n",
    "4. **Temps total (barres simples)** :\n",
    "   - Compare le temps cumul√© des op√©rations\n",
    "   - Facilite la comparaison globale\n",
    "\n",
    "5. **Score d'efficacit√© (barres)** :\n",
    "   - Calcule un score composite bas√© sur tous les crit√®res\n",
    "   - Permet une comparaison synth√©tique\n",
    "\n",
    "6. **R√©sum√© ex√©cutif (texte)** :\n",
    "   - Synth√®se textuelle des r√©sultats\n",
    "   - Recommandation finale\n",
    "   - Points cl√©s √† retenir"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6871257fcc719de4"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e94102ef94b79c00",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# üìä Visualisations des performances avec matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Configuration matplotlib pour de beaux graphiques\n",
    "plt.style.use('default')\n",
    "plt.rcParams['figure.figsize'] = (15, 10)\n",
    "plt.rcParams['font.size'] = 11\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"G√©n√©ration des visualisations\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "def format_size_mb(size_bytes):\n",
    "    \"\"\"Formate la taille en MB\"\"\"\n",
    "    return f'{size_bytes / (1024*1024):.1f} MB'\n",
    "\n",
    "def create_performance_dashboard():\n",
    "    \"\"\"Cr√©e un dashboard complet des performances\"\"\"\n",
    "\n",
    "    # Cr√©ation de la figure avec 6 sous-graphiques\n",
    "    fig, ((ax1, ax2, ax3), (ax4, ax5, ax6)) = plt.subplots(2, 3, figsize=(18, 12))\n",
    "\n",
    "    # === 1. Temps d'op√©ration (barres) ===\n",
    "    operations = ['√âcriture', 'Lecture']\n",
    "    csv_times = [csv_write_time, csv_read_time]\n",
    "    parquet_times = [parquet_write_time, parquet_read_time]\n",
    "\n",
    "    x = np.arange(len(operations))\n",
    "    width = 0.35\n",
    "\n",
    "    bars1 = ax1.bar(x - width/2, csv_times, width, label='CSV', color='orange', alpha=0.7)\n",
    "    bars2 = ax1.bar(x + width/2, parquet_times, width, label='Parquet', color='green', alpha=0.7)\n",
    "\n",
    "    ax1.set_xlabel('Op√©rations')\n",
    "    ax1.set_ylabel('Temps (secondes)')\n",
    "    ax1.set_title('Temps d\\'op√©ration')\n",
    "    ax1.set_xticks(x)\n",
    "    ax1.set_xticklabels(operations)\n",
    "    ax1.legend()\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "\n",
    "    for bar in bars1:\n",
    "        height = bar.get_height()\n",
    "        ax1.text(bar.get_x() + bar.get_width()/2., height + 2,\n",
    "                 f'{height:.1f}s', ha='center', va='bottom', fontweight='bold')\n",
    "\n",
    "    for bar in bars2:\n",
    "        height = bar.get_height()\n",
    "        ax1.text(bar.get_x() + bar.get_width()/2., height + 2,\n",
    "                 f'{height:.1f}s', ha='center', va='bottom', fontweight='bold')\n",
    "\n",
    "    # === 2. R√©partition de l'espace disque (secteurs) ===\n",
    "    sizes = [csv_size, parquet_size]\n",
    "    labels = ['CSV', 'Parquet']\n",
    "    colors = ['orange', 'green']\n",
    "\n",
    "    wedges, texts, autotexts = ax2.pie(sizes, labels=labels, colors=colors,\n",
    "                                       autopct='%1.1f%%', startangle=90)\n",
    "    ax2.set_title('Espace disque utilis√©')\n",
    "\n",
    "    legend_labels = [f'{label}: {format_size_mb(size)}' for label, size in zip(labels, sizes)]\n",
    "    ax2.legend(wedges, legend_labels, loc=\"center left\", bbox_to_anchor=(1, 0, 0.5, 1))\n",
    "\n",
    "    # === 3. Gains de performance ===\n",
    "    write_gain = ((csv_write_time - parquet_write_time) / csv_write_time * 100)\n",
    "    read_gain = ((csv_read_time - parquet_read_time) / csv_read_time * 100)\n",
    "    size_gain = ((csv_size - parquet_size) / csv_size * 100)\n",
    "    total_gain = (((csv_write_time + csv_read_time) - (parquet_write_time + parquet_read_time)) /\n",
    "                  (csv_write_time + csv_read_time) * 100)\n",
    "\n",
    "    metrics = ['√âcriture', 'Lecture', 'Taille', 'Total']\n",
    "    gains = [write_gain, read_gain, size_gain, total_gain]\n",
    "    colors_gain = ['green' if g > 0 else 'red' for g in gains]\n",
    "\n",
    "    bars = ax3.bar(metrics, gains, color=colors_gain, alpha=0.7)\n",
    "    ax3.set_ylabel('Gain (%)')\n",
    "    ax3.set_title('Gains Parquet vs CSV')\n",
    "    ax3.axhline(y=0, color='black', linestyle='-', linewidth=0.8)\n",
    "    ax3.grid(True, alpha=0.3)\n",
    "\n",
    "    for bar, gain in zip(bars, gains):\n",
    "        height = bar.get_height()\n",
    "        ax3.text(bar.get_x() + bar.get_width()/2.,\n",
    "                 height + (1 if height > 0 else -2),\n",
    "                 f'{gain:+.1f}%', ha='center',\n",
    "                 va='bottom' if height > 0 else 'top',\n",
    "                 fontweight='bold')\n",
    "\n",
    "    # === 4. Comparaison temps total ===\n",
    "    total_times = [csv_write_time + csv_read_time, parquet_write_time + parquet_read_time]\n",
    "    formats = ['CSV', 'Parquet']\n",
    "    colors_total = ['orange', 'green']\n",
    "\n",
    "    bars = ax4.bar(formats, total_times, color=colors_total, alpha=0.7)\n",
    "    ax4.set_ylabel('Temps total (secondes)')\n",
    "    ax4.set_title('Temps total (√âcriture + Lecture)')\n",
    "    ax4.grid(True, alpha=0.3)\n",
    "\n",
    "    for bar, time in zip(bars, total_times):\n",
    "        height = bar.get_height()\n",
    "        ax4.text(bar.get_x() + bar.get_width()/2., height + 5,\n",
    "                 f'{height:.1f}s', ha='center', va='bottom', fontweight='bold')\n",
    "\n",
    "    # === 5. Efficacit√© relative ===\n",
    "    csv_score = 1000 / (csv_write_time + csv_read_time + csv_size/(1024**2))\n",
    "    parquet_score = 1000 / (parquet_write_time + parquet_read_time + parquet_size/(1024**2))\n",
    "\n",
    "    scores = [csv_score, parquet_score]\n",
    "\n",
    "    bars = ax5.bar(formats, scores, color=colors_total, alpha=0.7)\n",
    "    ax5.set_ylabel('Score d\\'efficacit√©')\n",
    "    ax5.set_title('Score d\\'efficacit√© global')\n",
    "    ax5.grid(True, alpha=0.3)\n",
    "\n",
    "    for bar, score in zip(bars, scores):\n",
    "        height = bar.get_height()\n",
    "        ax5.text(bar.get_x() + bar.get_width()/2., height + 0.1,\n",
    "                 f'{score:.1f}', ha='center', va='bottom', fontweight='bold')\n",
    "\n",
    "    # === 6. R√©sum√© textuel ===\n",
    "    ax6.axis('off')\n",
    "\n",
    "    summary = f\"\"\"R√âSUM√â EX√âCUTIF\n",
    "    \n",
    "TEMPS (secondes)\n",
    "√âcriture: CSV {csv_write_time:.1f}s vs Parquet {parquet_write_time:.1f}s\n",
    "Lecture:  CSV {csv_read_time:.1f}s vs Parquet {parquet_read_time:.1f}s\n",
    "\n",
    "ESPACE DISQUE\n",
    "CSV:     {format_size_mb(csv_size)}\n",
    "Parquet: {format_size_mb(parquet_size)}\n",
    "\n",
    "GAINS PARQUET\n",
    "Temps total: {total_gain:+.1f}%\n",
    "Espace:      {size_gain:+.1f}%\n",
    "\n",
    "VERDICT\n",
    "Recommandation: {'PARQUET' if total_gain > 0 and size_gain > 0 else 'CSV'}\n",
    "\n",
    "Parquet excelle en:\n",
    "‚Ä¢ Compression des donn√©es\n",
    "‚Ä¢ Vitesse de lecture\n",
    "‚Ä¢ Performance analytique\"\"\"\n",
    "\n",
    "    ax6.text(0.05, 0.95, summary, transform=ax6.transAxes, fontsize=10,\n",
    "             verticalalignment='top', fontfamily='monospace',\n",
    "             bbox=dict(boxstyle=\"round,pad=0.5\", facecolor=\"lightblue\", alpha=0.8))\n",
    "\n",
    "    plt.suptitle('Dashboard Performance: CSV vs Parquet\\nDataset OpenFoodFacts (3.9M lignes)',\n",
    "                 fontsize=14, fontweight='bold', y=0.98)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.subplots_adjust(top=0.93)\n",
    "    plt.show()\n",
    "\n",
    "# G√©n√©ration du dashboard\n",
    "create_performance_dashboard()\n",
    "\n",
    "print(\"Visualisations g√©n√©r√©es avec succ√®s!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
