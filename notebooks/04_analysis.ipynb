{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6ba7aba4",
   "metadata": {},
   "source": [
    "# üìä 04 - Analyse de Donn√©es avec Spark\n",
    "Ce notebook a pour objectif de fournir des donn√©es d'analyse √† partir des donn√©es d√©j√† transform√©es, nettoy√©es et enrichies.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "227980a4-00b6-4f3b-b6b3-fd51250c1e29",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# 0. Stoppe toute session existante\n",
    "try:\n",
    "    spark.stop()\n",
    "except:\n",
    "    pass\n",
    "\n",
    "# 1. Recr√©ation SparkSession avec 16 Go de RAM allou√©s\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = (\n",
    "    SparkSession.builder\n",
    "        .appName(\"04_analyse\")\n",
    "        .master(\"local[*]\")  # Utilise tous les c≈ìurs disponibles\n",
    "        .config(\"spark.hadoop.fs.defaultFS\", \"file:///\")\n",
    "        .config(\"spark.driver.host\", \"127.0.0.1\")\n",
    "        .config(\"spark.driver.bindAddress\", \"0.0.0.0\")\n",
    "        .config(\"spark.driver.memory\", \"16g\")         # <-- alloue 16 Go au driver Spark\n",
    "        .config(\"spark.executor.memory\", \"16g\")       # <-- alloue 16 Go aux t√¢ches ex√©cut√©es (optionnel en local)\n",
    "        .config(\"spark.sql.shuffle.partitions\", \"8\")  # <-- limite les partitions pour limiter le thread/m√©moire\n",
    "        .getOrCreate()\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "65f9d412",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, lower, trim, count\n",
    "import os\n",
    "\n",
    "# 1. Chargement\n",
    "path = os.path.abspath(os.path.join(os.getcwd(), \"../data/step3_enriched_csv\"))\n",
    "df_enriched = spark.read.option(\"header\", \"true\").option(\"sep\", \";\").csv(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c84938c7-9644-4a48-a095-6964c53a5855",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Marques les plus fr√©quentes\n",
    "top_brands = df_enriched.groupBy(\"brands\") \\\n",
    "    .count() \\\n",
    "    .orderBy(\"count\", ascending=False) \\\n",
    "    .filter(col(\"brands\").isNotNull() & (col(\"brands\") != \"\")) \\\n",
    "    .limit(30)\n",
    "\n",
    "# Sauvegarde pour la visualisation\n",
    "top_brands.coalesce(1) \\\n",
    "    .write.option(\"header\", \"true\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .csv(\"../data/viz/top_brands\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fb54ef92-9ad3-4035-a391-63241d896d75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pays les plus fr√©quents\n",
    "top_countries = df_enriched.groupBy(\"country\") \\\n",
    "    .count() \\\n",
    "    .orderBy(\"count\", ascending=False) \\\n",
    "    .filter(col(\"country\").isNotNull() & (col(\"country\") != \"\")) \\\n",
    "    .limit(20)\n",
    "\n",
    "top_countries.coalesce(1) \\\n",
    "    .write.option(\"header\", \"true\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .csv(\"../data/viz/top_countries\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "23bb50a5-ae14-48d1-9927-24ec80d0d6ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üìä Moyenne score_env_composite_flexible (impact environnementale) par pays (class√©s par fr√©quence d‚Äôapparition)\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "# Fr√©quence des pays\n",
    "country_freq = df_enriched.groupBy(\"country\").count()\n",
    "\n",
    "# Moyenne des scores environnementaux\n",
    "score_by_country = df_enriched.groupBy(\"country\") \\\n",
    "    .agg(F.avg(\"score_env_composite_flexible\").alias(\"avg_score_env_composite_flexible\"))\n",
    "\n",
    "# Jointure et tri par fr√©quence d√©croissante\n",
    "result_country = score_by_country.join(country_freq, on=\"country\") \\\n",
    "    .orderBy(F.desc(\"count\"))\n",
    "\n",
    "result_country.coalesce(1) \\\n",
    "    .write.option(\"header\", \"true\") \\\n",
    "    .option(\"sep\", \";\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .csv(\"../data/viz/score_env_by_country\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d7a0af72-952a-4e1c-9cd5-2590cf249643",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Moyenne score_composite (aliments sains) par pays (class√©s par fr√©quence d‚Äôapparition) \n",
    "\n",
    "score_composite_by_country = df_enriched.groupBy(\"country\") \\\n",
    "    .agg(F.avg(\"score_composite\").alias(\"avg_score_composite\"))\n",
    "\n",
    "result_composite_country = score_composite_by_country.join(country_freq, on=\"country\") \\\n",
    "    .orderBy(F.desc(\"count\"))\n",
    "\n",
    "result_composite_country.coalesce(1) \\\n",
    "    .write.option(\"header\", \"true\") \\\n",
    "    .option(\"sep\", \";\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .csv(\"../data/viz/score_composite_by_country\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "87d2cd37-d351-4ad7-b4be-5a7b3aad6b14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üè∑Ô∏è Moyenne score_env_composite_flexible par marque (class√©es par fr√©quence)\n",
    "\n",
    "brand_freq = df_enriched.groupBy(\"brands\").count()\n",
    "\n",
    "score_by_brand = df_enriched.groupBy(\"brands\") \\\n",
    "    .agg(F.avg(\"score_env_composite_flexible\").alias(\"avg_score_env_composite_flexible\"))\n",
    "\n",
    "result_brand = score_by_brand.join(brand_freq, on=\"brands\") \\\n",
    "    .orderBy(F.desc(\"count\"))\n",
    "\n",
    "result_brand.coalesce(1) \\\n",
    "    .write.option(\"header\", \"true\") \\\n",
    "    .option(\"sep\", \";\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .csv(\"../data/viz/score_env_by_brand\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "820e8c34-5fd8-4fc5-8c3f-bd310fb92421",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üè∑Ô∏è Moyenne score_composite par marque (class√©es par fr√©quence)\n",
    "\n",
    "score_composite_by_brand = df_enriched.groupBy(\"brands\") \\\n",
    "    .agg(F.avg(\"score_composite\").alias(\"avg_score_composite\"))\n",
    "\n",
    "result_composite_brand = score_composite_by_brand.join(brand_freq, on=\"brands\") \\\n",
    "    .orderBy(F.desc(\"count\"))\n",
    "\n",
    "result_composite_brand.coalesce(1) \\\n",
    "    .write.option(\"header\", \"true\") \\\n",
    "    .option(\"sep\", \";\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .csv(\"../data/viz/score_composite_by_brand\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e538c60f-1716-45b6-8054-8aed4f53660a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Colonnes bool√©ennes d√©tect√©es (6) : ['is_vegan', 'is_vegetarian', 'is_sans_sucre', 'is_protein_plus', 'is_light', 'is_ultra_transformed']\n",
      "üìã V√©rification finale des nulls :\n",
      " - sum(has_vegan): 0 ligne(s) nulles\n",
      " - sum(has_vegetarian): 0 ligne(s) nulles\n",
      " - sum(has_sans_sucre): 0 ligne(s) nulles\n",
      " - sum(has_protein_plus): 0 ligne(s) nulles\n",
      " - sum(has_light): 0 ligne(s) nulles\n",
      " - sum(has_ultra_transformed): 0 ligne(s) nulles\n",
      "‚úÖ Nombre de lignes dans df_final : 684661\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, avg, max as spark_max, when, lit\n",
    "from pyspark.sql.types import BooleanType\n",
    "\n",
    "# üß± 1. Base des m√©triques quantitatives\n",
    "metrics_cols = [\n",
    "    col(\"score_env_composite_flexible\").cast(\"double\").alias(\"score_env_composite_flexible\"),\n",
    "    col(\"score_composite\").cast(\"double\").alias(\"score_composite\"),\n",
    "    col(\"energy-kcal_100g\").cast(\"double\").alias(\"energy_kcal_100g\")\n",
    "]\n",
    "\n",
    "# üîé 2. Colonnes bool√©ennes personnalis√©es\n",
    "bool_cols = [c for c in df_enriched.columns if c.startswith(\"is_\")]\n",
    "print(f\"‚úÖ Colonnes bool√©ennes d√©tect√©es ({len(bool_cols)}) :\", bool_cols)\n",
    "\n",
    "# üßº 3. Forcer le cast vers bool√©en + nulls ‚Üí False\n",
    "df_clean = df_enriched\n",
    "for c in bool_cols:\n",
    "    df_clean = df_clean.withColumn(\n",
    "        c,\n",
    "        when(col(c).cast(BooleanType()).isNull(), lit(False)).otherwise(col(c).cast(BooleanType()))\n",
    "    )\n",
    "\n",
    "# üßÆ 4. Cr√©ation de la base avec cast vers int pour aggregation\n",
    "df_base = df_clean.select(\n",
    "    col(\"country\"), col(\"brands\"),\n",
    "    *metrics_cols,\n",
    "    *[col(c).cast(\"int\").alias(c) for c in bool_cols]\n",
    ")\n",
    "df_base.createOrReplaceTempView(\"df_base\")\n",
    "\n",
    "# üìä 5. G√©n√©ration dynamique de la requ√™te CUBE\n",
    "base_expr = \"\"\"\n",
    "    country,\n",
    "    brands,\n",
    "    AVG(score_env_composite_flexible) AS avg_env_score,\n",
    "    AVG(score_composite) AS avg_composite_score,\n",
    "    AVG(energy_kcal_100g) AS avg_kcal,\n",
    "    GROUPING(country) AS grouping_country,\n",
    "    GROUPING(brands) AS grouping_brands,\n",
    "    GROUPING_ID(country, brands) AS grouping_id\n",
    "\"\"\"\n",
    "bool_exprs = \",\\n    \".join([\n",
    "    f\"MAX({c}) = 1 AS has_{c.replace('is_', '')}\" for c in bool_cols\n",
    "])\n",
    "query = f\"\"\"\n",
    "SELECT\n",
    "    {base_expr},\n",
    "    {bool_exprs}\n",
    "FROM df_base\n",
    "GROUP BY CUBE(country, brands)\n",
    "\"\"\"\n",
    "\n",
    "df_final = spark.sql(query)\n",
    "\n",
    "# ‚úÖ V√©rification\n",
    "has_cols = [f\"has_{c.replace('is_', '')}\" for c in bool_cols]\n",
    "null_counts = df_final.select([\n",
    "    col(c).isNull().cast(\"int\").alias(c + \"_nulls\") for c in has_cols\n",
    "]).groupBy().sum().collect()[0].asDict()\n",
    "\n",
    "print(\"üìã V√©rification finale des nulls :\")\n",
    "for k, v in null_counts.items():\n",
    "    print(f\" - {k.replace('_nulls','')}: {v} ligne(s) nulles\")\n",
    "\n",
    "# Remplacement √©ventuel des derniers nulls\n",
    "from pyspark.sql.functions import coalesce\n",
    "for c in has_cols:\n",
    "    df_final = df_final.withColumn(c, coalesce(col(c), lit(False)))\n",
    "\n",
    "# üíæ Export final\n",
    "df_final.cache()\n",
    "print(\"‚úÖ Nombre de lignes dans df_final :\", df_final.count())\n",
    "\n",
    "df_final.coalesce(1) \\\n",
    "    .write.option(\"header\", \"true\") \\\n",
    "    .option(\"sep\", \";\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .csv(\"../data/viz/cube_agg_country_brand_scores_enriched\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
