{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6ba7aba4",
   "metadata": {},
   "source": [
    "# üìä 04 - Analyse de Donn√©es avec Spark\n",
    "Ce notebook a pour objectif de fournir des donn√©es d'analyse √† partir des donn√©es d√©j√† transform√©es, nettoy√©es et enrichies.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "227980a4-00b6-4f3b-b6b3-fd51250c1e29",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# 0. Stoppe toute session existante\n",
    "try:\n",
    "    spark.stop()\n",
    "except:\n",
    "    pass\n",
    "\n",
    "# 1. Recr√©ation SparkSession avec 16 Go de RAM allou√©s\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = (\n",
    "    SparkSession.builder\n",
    "        .appName(\"04_analyse\")\n",
    "        .master(\"local[*]\")  # Utilise tous les c≈ìurs disponibles\n",
    "        .config(\"spark.hadoop.fs.defaultFS\", \"file:///\")\n",
    "        .config(\"spark.driver.host\", \"127.0.0.1\")\n",
    "        .config(\"spark.driver.bindAddress\", \"0.0.0.0\")\n",
    "        .config(\"spark.driver.memory\", \"16g\")         # <-- alloue 16 Go au driver Spark\n",
    "        .config(\"spark.executor.memory\", \"16g\")       # <-- alloue 16 Go aux t√¢ches ex√©cut√©es (optionnel en local)\n",
    "        .config(\"spark.sql.shuffle.partitions\", \"8\")  # <-- limite les partitions pour limiter le thread/m√©moire\n",
    "        .getOrCreate()\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "65f9d412",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, lower, trim, count\n",
    "import os\n",
    "\n",
    "# 1. Chargement\n",
    "path = os.path.abspath(os.path.join(os.getcwd(), \"../data/step3_enriched_csv\"))\n",
    "df_enriched = spark.read.option(\"header\", \"true\").option(\"sep\", \";\").csv(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c84938c7-9644-4a48-a095-6964c53a5855",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Marques les plus fr√©quentes\n",
    "top_brands = df_enriched.groupBy(\"brands\") \\\n",
    "    .count() \\\n",
    "    .orderBy(\"count\", ascending=False) \\\n",
    "    .filter(col(\"brands\").isNotNull() & (col(\"brands\") != \"\")) \\\n",
    "    .limit(30)\n",
    "\n",
    "# Sauvegarde pour la visualisation\n",
    "top_brands.coalesce(1) \\\n",
    "    .write.option(\"header\", \"true\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .csv(\"../data/viz/top_brands\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fb54ef92-9ad3-4035-a391-63241d896d75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pays les plus fr√©quents\n",
    "top_countries = df_enriched.groupBy(\"country\") \\\n",
    "    .count() \\\n",
    "    .orderBy(\"count\", ascending=False) \\\n",
    "    .filter(col(\"country\").isNotNull() & (col(\"country\") != \"\")) \\\n",
    "    .limit(20)\n",
    "\n",
    "top_countries.coalesce(1) \\\n",
    "    .write.option(\"header\", \"true\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .csv(\"../data/viz/top_countries\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "23bb50a5-ae14-48d1-9927-24ec80d0d6ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üìä Moyenne score_env_composite_flexible (impact environnementale) par pays (class√©s par fr√©quence d‚Äôapparition)\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "# Fr√©quence des pays\n",
    "country_freq = df_enriched.groupBy(\"country\").count()\n",
    "\n",
    "# Moyenne des scores environnementaux\n",
    "score_by_country = df_enriched.groupBy(\"country\") \\\n",
    "    .agg(F.avg(\"score_env_composite_flexible\").alias(\"avg_score_env_composite_flexible\"))\n",
    "\n",
    "# Jointure et tri par fr√©quence d√©croissante\n",
    "result_country = score_by_country.join(country_freq, on=\"country\") \\\n",
    "    .orderBy(F.desc(\"count\"))\n",
    "\n",
    "result_country.coalesce(1) \\\n",
    "    .write.option(\"header\", \"true\") \\\n",
    "    .option(\"sep\", \";\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .csv(\"../data/viz/score_env_by_country\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d7a0af72-952a-4e1c-9cd5-2590cf249643",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Moyenne score_composite (aliments sains) par pays (class√©s par fr√©quence d‚Äôapparition) \n",
    "\n",
    "score_composite_by_country = df_enriched.groupBy(\"country\") \\\n",
    "    .agg(F.avg(\"score_composite\").alias(\"avg_score_composite\"))\n",
    "\n",
    "result_composite_country = score_composite_by_country.join(country_freq, on=\"country\") \\\n",
    "    .orderBy(F.desc(\"count\"))\n",
    "\n",
    "result_composite_country.coalesce(1) \\\n",
    "    .write.option(\"header\", \"true\") \\\n",
    "    .option(\"sep\", \";\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .csv(\"../data/viz/score_composite_by_country\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "87d2cd37-d351-4ad7-b4be-5a7b3aad6b14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üè∑Ô∏è Moyenne score_env_composite_flexible par marque (class√©es par fr√©quence)\n",
    "\n",
    "brand_freq = df_enriched.groupBy(\"brands\").count()\n",
    "\n",
    "score_by_brand = df_enriched.groupBy(\"brands\") \\\n",
    "    .agg(F.avg(\"score_env_composite_flexible\").alias(\"avg_score_env_composite_flexible\"))\n",
    "\n",
    "result_brand = score_by_brand.join(brand_freq, on=\"brands\") \\\n",
    "    .orderBy(F.desc(\"count\"))\n",
    "\n",
    "result_brand.coalesce(1) \\\n",
    "    .write.option(\"header\", \"true\") \\\n",
    "    .option(\"sep\", \";\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .csv(\"../data/viz/score_env_by_brand\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "820e8c34-5fd8-4fc5-8c3f-bd310fb92421",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üè∑Ô∏è Moyenne score_composite par marque (class√©es par fr√©quence)\n",
    "\n",
    "score_composite_by_brand = df_enriched.groupBy(\"brands\") \\\n",
    "    .agg(F.avg(\"score_composite\").alias(\"avg_score_composite\"))\n",
    "\n",
    "result_composite_brand = score_composite_by_brand.join(brand_freq, on=\"brands\") \\\n",
    "    .orderBy(F.desc(\"count\"))\n",
    "\n",
    "result_composite_brand.coalesce(1) \\\n",
    "    .write.option(\"header\", \"true\") \\\n",
    "    .option(\"sep\", \";\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .csv(\"../data/viz/score_composite_by_brand\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e538c60f-1716-45b6-8054-8aed4f53660a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Colonnes bool√©ennes d√©tect√©es (6) : ['is_vegan', 'is_vegetarian', 'is_sans_sucre', 'is_protein_plus', 'is_light', 'is_ultra_transformed']\n",
      "üìã V√©rification finale des nulls :\n",
      " - sum(has_vegan): 0 ligne(s) nulles\n",
      " - sum(has_vegetarian): 0 ligne(s) nulles\n",
      " - sum(has_sans_sucre): 0 ligne(s) nulles\n",
      " - sum(has_protein_plus): 0 ligne(s) nulles\n",
      " - sum(has_light): 0 ligne(s) nulles\n",
      " - sum(has_ultra_transformed): 0 ligne(s) nulles\n",
      "‚úÖ Nombre de lignes dans df_final : 684661\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, avg, max as spark_max, when, lit\n",
    "from pyspark.sql.types import BooleanType\n",
    "\n",
    "# üß± 1. Base des m√©triques quantitatives\n",
    "metrics_cols = [\n",
    "    col(\"score_env_composite_flexible\").cast(\"double\").alias(\"score_env_composite_flexible\"),\n",
    "    col(\"score_composite\").cast(\"double\").alias(\"score_composite\"),\n",
    "    col(\"energy-kcal_100g\").cast(\"double\").alias(\"energy_kcal_100g\")\n",
    "]\n",
    "\n",
    "# üîé 2. Colonnes bool√©ennes personnalis√©es\n",
    "bool_cols = [c for c in df_enriched.columns if c.startswith(\"is_\")]\n",
    "print(f\"‚úÖ Colonnes bool√©ennes d√©tect√©es ({len(bool_cols)}) :\", bool_cols)\n",
    "\n",
    "# üßº 3. Forcer le cast vers bool√©en + nulls ‚Üí False\n",
    "df_clean = df_enriched\n",
    "for c in bool_cols:\n",
    "    df_clean = df_clean.withColumn(\n",
    "        c,\n",
    "        when(col(c).cast(BooleanType()).isNull(), lit(False)).otherwise(col(c).cast(BooleanType()))\n",
    "    )\n",
    "\n",
    "# üßÆ 4. Cr√©ation de la base avec cast vers int pour aggregation\n",
    "df_base = df_clean.select(\n",
    "    col(\"country\"), col(\"brands\"),\n",
    "    *metrics_cols,\n",
    "    *[col(c).cast(\"int\").alias(c) for c in bool_cols]\n",
    ")\n",
    "df_base.createOrReplaceTempView(\"df_base\")\n",
    "\n",
    "# üìä 5. G√©n√©ration dynamique de la requ√™te CUBE\n",
    "base_expr = \"\"\"\n",
    "    country,\n",
    "    brands,\n",
    "    AVG(score_env_composite_flexible) AS avg_env_score,\n",
    "    AVG(score_composite) AS avg_composite_score,\n",
    "    AVG(energy_kcal_100g) AS avg_kcal,\n",
    "    GROUPING(country) AS grouping_country,\n",
    "    GROUPING(brands) AS grouping_brands,\n",
    "    GROUPING_ID(country, brands) AS grouping_id\n",
    "\"\"\"\n",
    "bool_exprs = \",\\n    \".join([\n",
    "    f\"MAX({c}) = 1 AS has_{c.replace('is_', '')}\" for c in bool_cols\n",
    "])\n",
    "query = f\"\"\"\n",
    "SELECT\n",
    "    {base_expr},\n",
    "    {bool_exprs}\n",
    "FROM df_base\n",
    "GROUP BY CUBE(country, brands)\n",
    "\"\"\"\n",
    "\n",
    "df_final = spark.sql(query)\n",
    "\n",
    "# ‚úÖ V√©rification\n",
    "has_cols = [f\"has_{c.replace('is_', '')}\" for c in bool_cols]\n",
    "null_counts = df_final.select([\n",
    "    col(c).isNull().cast(\"int\").alias(c + \"_nulls\") for c in has_cols\n",
    "]).groupBy().sum().collect()[0].asDict()\n",
    "\n",
    "print(\"üìã V√©rification finale des nulls :\")\n",
    "for k, v in null_counts.items():\n",
    "    print(f\" - {k.replace('_nulls','')}: {v} ligne(s) nulles\")\n",
    "\n",
    "# Remplacement √©ventuel des derniers nulls\n",
    "from pyspark.sql.functions import coalesce\n",
    "for c in has_cols:\n",
    "    df_final = df_final.withColumn(c, coalesce(col(c), lit(False)))\n",
    "\n",
    "# üíæ Export final\n",
    "df_final.cache()\n",
    "print(\"‚úÖ Nombre de lignes dans df_final :\", df_final.count())\n",
    "\n",
    "df_final.coalesce(1) \\\n",
    "    .write.option(\"header\", \"true\") \\\n",
    "    .option(\"sep\", \";\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .csv(\"../data/viz/cube_agg_country_brand_scores_enriched\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6e3cd557-137b-46f2-ad6f-9826dc8b66ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Colonnes num√©riques avec valeurs valides : ['serving_quantity', 'additives_n', 'nova_group', 'environmental_score_score', 'product_quantity', 'completeness', 'energy-kj_100g', 'energy-kcal_100g', 'energy-from-fat_100g', 'fat_100g', 'saturated-fat_100g', 'caprylic-acid_100g', 'capric-acid_100g', 'myristic-acid_100g', 'arachidic-acid_100g', 'lignoceric-acid_100g', 'cerotic-acid_100g', 'unsaturated-fat_100g', 'monounsaturated-fat_100g', 'polyunsaturated-fat_100g', 'omega-6-fat_100g', 'eicosapentaenoic-acid_100g', 'docosahexaenoic-acid_100g', 'dihomo-gamma-linolenic-acid_100g', 'oleic-acid_100g', 'mead-acid_100g', 'nervonic-acid_100g', 'trans-fat_100g', 'cholesterol_100g', 'carbohydrates_100g', 'sugars_100g', 'added-sugars_100g', 'sucrose_100g', 'glucose_100g', 'fructose_100g', 'galactose_100g', 'lactose_100g', 'starch_100g', 'polyols_100g', 'erythritol_100g', 'fiber_100g', 'soluble-fiber_100g', 'insoluble-fiber_100g', 'proteins_100g', 'casein_100g', 'serum-proteins_100g', 'nucleotides_100g', 'salt_100g', 'added-salt_100g', 'sodium_100g', 'alcohol_100g', 'vitamin-e_100g', 'vitamin-c_100g', 'vitamin-pp_100g', 'silica_100g', 'calcium_100g', 'phosphorus_100g', 'zinc_100g', 'caffeine_100g', 'taurine_100g', 'ph_100g', 'fruits-vegetables-nuts_100g', 'fruits-vegetables-nuts-dried_100g', 'fruits-vegetables-nuts-estimate_100g', 'fruits-vegetables-nuts-estimate-from-ingredients_100g', 'collagen-meat-protein-ratio_100g', 'cocoa_100g', 'chlorophyl_100g', 'carbon-footprint_100g', 'carbon-footprint-from-meat-or-fish_100g', 'nutrition-score-uk_100g', 'glycemic-index_100g', 'water-hardness_100g', 'choline_100g', 'beta-glucan_100g', 'carnitine_100g', 'acidity_100g', 'carbohydrates-total_100g', 'nutriscore_value', 'nova_score', 'score_composite', 'empreinte_carbone_score']\n",
      "‚úÖ Export termin√© : ../data/viz/univariate_profiling_export\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, mean, stddev, min, max, count, isnan, when, percentile_approx\n",
    "from pyspark.sql.types import NumericType, DoubleType\n",
    "from functools import reduce\n",
    "from math import ceil\n",
    "\n",
    "# üßº 0. Cast auto des strings contenant des chiffres\n",
    "for f in df_enriched.schema.fields:\n",
    "    if f.dataType.simpleString() == \"string\":\n",
    "        try:\n",
    "            sample = df_enriched.select(col(f.name)).dropna().limit(10).rdd.map(lambda row: row[0]).collect()\n",
    "            if all(isinstance(v, str) and v.replace(\",\", \".\").replace(\".\", \"\", 1).isdigit() for v in sample):\n",
    "                df_enriched = df_enriched.withColumn(f.name, col(f.name).cast(DoubleType()))\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "# üîé 1. D√©tection des colonnes num√©riques avec valeurs valides\n",
    "numeric_cols = [\n",
    "    f.name for f in df_enriched.schema.fields\n",
    "    if isinstance(f.dataType, NumericType)\n",
    "    and df_enriched.filter(~col(f.name).isNull() & ~isnan(col(f.name))).limit(1).count() > 0\n",
    "]\n",
    "print(f\"‚úÖ Colonnes num√©riques avec valeurs valides : {numeric_cols}\")\n",
    "\n",
    "if not numeric_cols:\n",
    "    print(\"‚ùå Aucune colonne num√©rique avec des valeurs valides.\")\n",
    "else:\n",
    "    # üìä 2a. Agr√©gats double\n",
    "    agg_exprs_double = []\n",
    "    for c in numeric_cols:\n",
    "        agg_exprs_double.extend([\n",
    "            mean(col(c)).alias(f\"{c}_mean\"),\n",
    "            stddev(col(c)).alias(f\"{c}_stddev\"),\n",
    "            min(col(c)).alias(f\"{c}_min\"),\n",
    "            max(col(c)).alias(f\"{c}_max\"),\n",
    "            percentile_approx(col(c), 0.5).alias(f\"{c}_median\"),\n",
    "        ])\n",
    "\n",
    "    # üìä 2b. Agr√©gats entier (bigint)\n",
    "    agg_exprs_int = []\n",
    "    for c in numeric_cols:\n",
    "        agg_exprs_int.extend([\n",
    "            count(when(col(c).isNull() | isnan(col(c)), c)).alias(f\"{c}_nulls\"),\n",
    "            count(col(c)).alias(f\"{c}_count\")\n",
    "        ])\n",
    "\n",
    "    df_double = df_enriched.agg(*agg_exprs_double)\n",
    "    df_int = df_enriched.agg(*agg_exprs_int)\n",
    "\n",
    "    def explode_columns(df):\n",
    "        chunk_size = 50\n",
    "        columns = df.columns\n",
    "        num_chunks = ceil(len(columns) / chunk_size)\n",
    "        escape = lambda c: f\"`{c.replace('`', '``')}`\"\n",
    "        parts = []\n",
    "        for i in range(num_chunks):\n",
    "            chunk = columns[i * chunk_size:(i + 1) * chunk_size]\n",
    "            stack_expr = f\"stack({len(chunk)}, \" + \", \".join([f\"'{c}', {escape(c)}\" for c in chunk]) + \") as (metric, value)\"\n",
    "            parts.append(df.selectExpr(stack_expr))\n",
    "        return reduce(lambda a, b: a.unionByName(b), parts)\n",
    "\n",
    "    profiling_long_double = explode_columns(df_double)\n",
    "    profiling_long_int = explode_columns(df_int)\n",
    "    profiling_long = profiling_long_double.unionByName(profiling_long_int)\n",
    "\n",
    "    # üíæ 5. Export\n",
    "    profiling_long.coalesce(1) \\\n",
    "        .write.option(\"header\", \"true\") \\\n",
    "        .option(\"sep\", \";\") \\\n",
    "        .mode(\"overwrite\") \\\n",
    "        .csv(\"../data/viz/univariate_profiling_export\")\n",
    "\n",
    "    print(\"‚úÖ Export termin√© : ../data/viz/univariate_profiling_export\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
